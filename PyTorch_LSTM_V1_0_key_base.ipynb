{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_LSTM_V1_0_key_base.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1NliHLfVy-a0N6Ok5K3HAny5_1nwBLTgJ","authorship_tag":"ABX9TyNgZ6VlU5oDC5e+W+gHua3C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install optuna"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALDi87Pc5OeI","executionInfo":{"status":"ok","timestamp":1655600601628,"user_tz":-540,"elapsed":9279,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"8da9ea1a-f93b-4d72-a1e0-d5a218456c08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting optuna\n","  Downloading optuna-2.10.1-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 14.7 MB/s \n","\u001b[?25hCollecting cliff\n","  Downloading cliff-3.10.1-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 12.8 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n","Collecting alembic\n","  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 105.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n","Collecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.37)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Collecting Mako\n","  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n","Collecting autopage>=0.4.0\n","  Downloading autopage-0.5.1-py3-none-any.whl (29 kB)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.9.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 84.6 MB/s \n","\u001b[?25hCollecting cmd2>=1.0.0\n","  Downloading cmd2-2.4.1-py3-none-any.whl (146 kB)\n","\u001b[K     |████████████████████████████████| 146 kB 81.7 MB/s \n","\u001b[?25hCollecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=cca1ec3cffe9f93239fee036b28b8c4cd626de1ed359e4df26b91fe2f4d18d15\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.2.0 alembic-1.8.0 autopage-0.5.1 cliff-3.10.1 cmaes-0.8.2 cmd2-2.4.1 colorlog-6.6.0 optuna-2.10.1 pbr-5.9.0 pyperclip-1.8.2 stevedore-3.5.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJ8QC3wS1Rd9","executionInfo":{"status":"ok","timestamp":1655602608143,"user_tz":-540,"elapsed":2250,"user":{"displayName":"이기","userId":"02906280205536551697"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"bfd23bc9-e7ce-48bd-8860-27ade6920883"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'220619_lstm_saved_model.pth'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings\n","\n","from torch.autograd import Variable\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter()\n","\n","import joblib\n","import pickle\n","import optuna\n","\n","warnings.filterwarnings('ignore')\n","%matplotlib inline\n","\n","from datetime import datetime\n","\n","datetime.today()\n","today = datetime.today()\n","str_today = today.strftime('%y%m%d')\n","save_model = str_today + '_lstm_saved_model.pth'\n","save_model"]},{"cell_type":"markdown","source":["### 함수정의"],"metadata":{"id":"C9_6GpfjIoAU"}},{"cell_type":"code","source":["def sliding_windows(data, lookback_length, forecast_length):\n","\n","    x = []\n","    y = []\n","    \n","    for i in range(lookback_length, len(data) - forecast_length + 1):\n","        _x = data[(i-lookback_length) : i]\n","        _y = data[i : (i + forecast_length)]\n","        x.append(_x)\n","        y.append(_y)\n","    return np.array(x), np.array(y)\n","\n","\n","def get_data_loader(X, y, batch_size):\n","\n","    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n","\n","    train_ds = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n","    train_dl = DataLoader(train_ds, batch_size = batch_size)\n","\n","    val_ds = TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n","    val_dl = DataLoader(val_ds, batch_size = batch_size)\n","\n","    input_size = x_train.shape[-1]\n","\n","    return train_dl, val_dl, input_size"],"metadata":{"id":"N7ckUYfEZ2tP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data loading && Preproces"],"metadata":{"id":"BCbnHbPQGyJI"}},{"cell_type":"code","source":["with open('/content/drive/MyDrive/Colab Notebooks/실무인증/Data/crypto_currency_data_key_v2.pickle', 'rb') as f:\n","    data = pickle.load(f)\n","\n","\n","data = data[['date', 'trade_price']]\n","data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n","data = data.sort_values(by='date').reset_index(drop=True)\n","\n","scaler = MinMaxScaler()\n","scale_cols = ['trade_price']\n","\n","# Loockback_period & forecasting_period\n","max_prediction_length = 20\n","lookback_length = 90\n","forecast_length = 1\n","training_data_max = len(data) - max_prediction_length\n","\n","\n","# 학습용 데이터\n","data_p = data.iloc[:training_data_max, :]\n","training_data = scaler.fit_transform(data_p[scale_cols])\n","\n"],"metadata":{"id":"6h3QCVaQ1oFF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Metric 생성을 위한 oot sample 정의"],"metadata":{"id":"840HR1ojHV0l"}},{"cell_type":"code","source":["# max_prediction_length 만큼의 데이터는 예측 데이터와 비교를 위해 분리\n","# actual_data = data.loc[~data.index.isin(data_p.index)][scale_cols]\n","# actual_data.shape\n","# Training set에 없는 데이터로 구성\n","# Input과 output의 pair로 정의\n","x_for_metric = scaler.fit_transform(data[training_data_max -lookback_length : training_data_max][scale_cols])\n","y_for_metric = scaler.fit_transform(data[training_data_max:][scale_cols])"],"metadata":{"id":"Dv736mbRHfZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM은 1 step 뒤의 값만을 예측하므로, forecasting_period를 1로 두고 진행\n","x, y = sliding_windows(training_data, lookback_length, forecast_length)"],"metadata":{"id":"nBuRbPSEIXzM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rL7UcE3uI3s-","executionInfo":{"status":"ok","timestamp":1655605783686,"user_tz":-540,"elapsed":8,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"1b36f1e3-6ccb-443c-8d2b-a547e9c4b240"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1597, 90, 1)"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["### Model정의"],"metadata":{"id":"LFTGRPdiI4Na"}},{"cell_type":"code","source":["class LSTM(nn.Module):\n","    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n","        super(LSTM, self).__init__()\n","        self.num_classes = num_classes\n","        self.num_layers = num_layers\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        \n","        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n","                            num_layers = num_layers, batch_first = True)\n","        \n","        self.fc = nn.Linear(hidden_size * num_layers , num_classes)\n","        \n","    def forward(self, x):\n","        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n","        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n","        \n","        # Propagate input through LSTM\n","        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n","        h_out = h_out.view(-1, self.hidden_size * self.num_layers)\n","        \n","        out = self.fc(h_out)\n","        \n","        return out"],"metadata":{"id":"oBWe5Qb9RIin"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(log_interval, model, train_dl, val_dl, optimizer, criterion, epoch):\n","\n","    best_loss = np.inf\n","    for epoch in range(epoch):\n","        train_loss = 0.0\n","        model.train()\n","        for data, target in train_dl:\n","\n","            if torch.cuda.is_available():\n","                data, target = data.cuda(), target.cuda()\n","                model = model.cuda()\n","\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = criterion(output, target) # mean-squared error for regression\n","            writer.add_scalar(\"Loss/train\", loss, epoch)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        # validation\n","        valid_loss = 0.0\n","        model.eval()\n","        for data, target in val_dl:\n","\n","            if torch.cuda.is_available():\n","                data, target = data.cuda(), target.cuda()\n","\n","            output = model(data)\n","            loss = criterion(output, target)\n","            valid_loss += loss.item()\n","\n","        if ( epoch % log_interval == 0 ):\n","            print(f'\\n Epoch {epoch} \\t Training Loss: {train_loss / len(train_dl)} \\t Validation Loss: {valid_loss / len(val_dl)} \\n')\n","\n","        if best_loss > (valid_loss / len(val_dl)):\n","            print(f'Validation Loss Decreased({best_loss:.6f}--->{(valid_loss / len(val_dl)):.6f}) \\t Saving The Model')\n","            best_loss = (valid_loss / len(val_dl))\n","            torch.save(model.state_dict(), save_model)\n","            \n","    writer.close()\n","    return best_loss\n","\n","\n","def smape(a, f):\n","    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)"],"metadata":{"id":"R1qLb-u0eOj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aa = x_for_metric\n","tmp = np.append( np.expand_dims(aa[1:, :], 0), np.expand_dims(y_for_metric[2, :], (0,2)), axis=1)\n","tmp.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdZbju6AJzAN","executionInfo":{"status":"ok","timestamp":1655602684513,"user_tz":-540,"elapsed":6,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"5a370208-e562-4c7b-f30a-c64e85296e57"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1, 90, 1)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["def objective(trial):\n","\n","    cfg = { 'batch_size' : trial.suggest_categorical('batch_size',[64, 128, 256, 512]),\n","            'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-4, 1e-0), #trial.suggest_loguniform('learning_rate', 1e-2, 1e-1), # learning rate을 0.01-0.1까지 로그 uniform 분포로 사용          \n","            # 'activation': trial.suggest_categorical('activation',[ torch.nn.relu6, torch.nn.tanh ]),\n","            'hidden_size': trial.suggest_categorical('hidden_size', [16, 32, 64,128,256,512,1024]),\n","            'num_layers': trial.suggest_int('num_layers', 1, 5, 1) }\n","\n","    torch.manual_seed(42)\n","\n","    log_interval = 10\n","    num_classes = 1\n","    num_epochs = 150\n","\n","    train_dl, val_dl, input_size = get_data_loader(x, y,  cfg['batch_size'])\n","\n","    model = LSTM(num_classes=num_classes, \n","                 input_size=input_size, \n","                 hidden_size=cfg['hidden_size'], \n","                 num_layers=cfg['num_layers'])\n","\n","    if torch.cuda.is_available():\n","        model = model.cuda()\n","\n","    optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n","    criterion = torch.nn.MSELoss()\n","    best_loss = train(log_interval, model, train_dl, val_dl, optimizer, criterion,  num_epochs)\n","\n","    print('best loss for the trial = ', best_loss)\n","    predict_data = []\n","\n","    # 여기서 x는 (sample, lookback_length, 1)의 크기를 지님. 따라서, 제일 앞의 시점을 제거하려면, x[:, -1, :]이 되어야 함\n","    x_pred = np.expand_dims(x_for_metric, 0)  # Inference에 사용할 lookback data를 x_pred로 지정. 앞으로 x_pred를 하나씩 옮겨 가면서 inference를 할 예정\n","\n","    # print('-----------------------y shape before loop = ', y.shape)\n","    for j, i in enumerate(range(max_prediction_length)):\n","\n","        # feed the last forecast back to the model as an input\n","        x_pred = np.append( x_pred[:, 1:, :], np.expand_dims(y_for_metric[j, :], (0,2)), axis=1)\n","\n","        # print(f'After update data = {x_pred.shape}')\n","        xt_pred = torch.Tensor(x_pred)\n","\n","        if torch.cuda.is_available():\n","            xt_pred = xt_pred.cuda()\n","\n","        # generate the next forecast\n","        yt_pred = model(xt_pred)\n","\n","        # print(f'model result yt_pred = {yt_pred.shape}')\n","        # x_pred = xt_pred.cpu().detach().numpy()\n","        y_pred = yt_pred.cpu().detach().numpy()\n","\n","        # save the forecast\n","        predict_data.append(y_pred)\n","\n","    # transform the forecasts back to the original scale\n","    predict_data = np.array(predict_data).reshape(-1, 1)\n","    SMAPE = smape(y_for_metric, predict_data)\n","    print(f' \\nSMAPE : {SMAPE}')\n","\n","\n","    return SMAPE\n"],"metadata":{"id":"WYyF-qAseVWf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sampler = optuna.samplers.TPESampler()\n","#   sampler = optuna.samplers.SkoptSampler()\n","\n","# model.load_state_dict(torch.load('lstm_saved_model.pth'))\n","    \n","study = optuna.create_study(sampler=sampler, direction='minimize')\n","study.optimize(objective, n_trials=25)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-SZ9QNcBj6qX","outputId":"8833b380-37e6-48ea-a51d-b76fa2e6f6c9","executionInfo":{"status":"ok","timestamp":1655607767726,"user_tz":-540,"elapsed":1963325,"user":{"displayName":"이기","userId":"02906280205536551697"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:30:05,544]\u001b[0m A new study created in memory with name: no-name-e13b3d26-8d6d-4fea-8fef-815fc55849ca\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0 \t Training Loss: 15.178350023180247 \t Validation Loss: 0.4323301464319229 \n","\n","Validation Loss Decreased(inf--->0.432330) \t Saving The Model\n","Validation Loss Decreased(0.432330--->0.055732) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.10293635167181492 \t Validation Loss: 0.38503365218639374 \n","\n","\n"," Epoch 20 \t Training Loss: 0.03569756089709699 \t Validation Loss: 0.263226680457592 \n","\n","\n"," Epoch 30 \t Training Loss: 0.05523619893938303 \t Validation Loss: 0.2687242478132248 \n","\n","\n"," Epoch 40 \t Training Loss: 0.08750697299838066 \t Validation Loss: 0.25205789506435394 \n","\n","\n"," Epoch 50 \t Training Loss: 0.18778817653656005 \t Validation Loss: 0.1341383494436741 \n","\n","\n"," Epoch 60 \t Training Loss: 1.4647148534655572 \t Validation Loss: 0.24524462223052979 \n","\n","\n"," Epoch 70 \t Training Loss: 0.07659415751695633 \t Validation Loss: 0.3233623802661896 \n","\n","\n"," Epoch 80 \t Training Loss: 0.07906041136011481 \t Validation Loss: 0.27640412747859955 \n","\n","\n"," Epoch 90 \t Training Loss: 0.09463043054565787 \t Validation Loss: 0.2759473919868469 \n","\n","\n"," Epoch 100 \t Training Loss: 0.10983152762055397 \t Validation Loss: 0.2743658870458603 \n","\n","\n"," Epoch 110 \t Training Loss: 0.12223460115492343 \t Validation Loss: 0.29046814143657684 \n","\n","\n"," Epoch 120 \t Training Loss: 0.19465303421020508 \t Validation Loss: 0.32768605649471283 \n","\n","\n"," Epoch 130 \t Training Loss: 0.6873726367950439 \t Validation Loss: 0.6379031836986542 \n","\n","\n"," Epoch 140 \t Training Loss: 0.2360733062028885 \t Validation Loss: 0.40549130737781525 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:30:31,862]\u001b[0m Trial 0 finished with value: 53.985243659366795 and parameters: {'batch_size': 256, 'learning_rate': 0.03349119334660142, 'hidden_size': 256, 'num_layers': 2}. Best is trial 0 with value: 53.985243659366795.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.05573238432407379\n"," \n","SMAPE : 53.985243659366795\n","\n"," Epoch 0 \t Training Loss: 57.72512938603759 \t Validation Loss: 15.210519790649414 \n","\n","Validation Loss Decreased(inf--->15.210520) \t Saving The Model\n","Validation Loss Decreased(15.210520--->4.394510) \t Saving The Model\n","Validation Loss Decreased(4.394510--->0.556240) \t Saving The Model\n","Validation Loss Decreased(0.556240--->0.451122) \t Saving The Model\n","Validation Loss Decreased(0.451122--->0.260542) \t Saving The Model\n","Validation Loss Decreased(0.260542--->0.090948) \t Saving The Model\n","Validation Loss Decreased(0.090948--->0.064261) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.05674657877534628 \t Validation Loss: 0.11350652575492859 \n","\n","\n"," Epoch 20 \t Training Loss: 0.08574784155935049 \t Validation Loss: 0.13282871743043265 \n","\n","Validation Loss Decreased(0.064261--->0.064046) \t Saving The Model\n","Validation Loss Decreased(0.064046--->0.046219) \t Saving The Model\n","Validation Loss Decreased(0.046219--->0.031289) \t Saving The Model\n","Validation Loss Decreased(0.031289--->0.024428) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.3661614032462239 \t Validation Loss: 0.03275031968951225 \n","\n","\n"," Epoch 40 \t Training Loss: 0.8118056155741215 \t Validation Loss: 0.9389104247093201 \n","\n","\n"," Epoch 50 \t Training Loss: 0.26525715524330734 \t Validation Loss: 0.07436669990420341 \n","\n","Validation Loss Decreased(0.024428--->0.024415) \t Saving The Model\n","Validation Loss Decreased(0.024415--->0.022402) \t Saving The Model\n","Validation Loss Decreased(0.022402--->0.021394) \t Saving The Model\n","Validation Loss Decreased(0.021394--->0.021052) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0992712281178683 \t Validation Loss: 0.021572642882044118 \n","\n","\n"," Epoch 70 \t Training Loss: 0.0745965451002121 \t Validation Loss: 0.03289589347938696 \n","\n","\n"," Epoch 80 \t Training Loss: 0.07610168182291091 \t Validation Loss: 0.026724386649827164 \n","\n","Validation Loss Decreased(0.021052--->0.020970) \t Saving The Model\n","Validation Loss Decreased(0.020970--->0.020846) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.10713155281264335 \t Validation Loss: 0.02099572711934646 \n","\n","\n"," Epoch 100 \t Training Loss: 0.2009678902104497 \t Validation Loss: 0.06273654848337173 \n","\n","\n"," Epoch 110 \t Training Loss: 0.3824029905023053 \t Validation Loss: 0.2638138433297475 \n","\n","\n"," Epoch 120 \t Training Loss: 0.5056568623345811 \t Validation Loss: 0.4307164053122203 \n","\n","\n"," Epoch 130 \t Training Loss: 0.5298742149723694 \t Validation Loss: 0.4631356696287791 \n","\n","\n"," Epoch 140 \t Training Loss: 0.5341765037795995 \t Validation Loss: 0.46942397952079773 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:31:56,368]\u001b[0m Trial 1 finished with value: 134.8104958175032 and parameters: {'batch_size': 128, 'learning_rate': 0.03844704821982354, 'hidden_size': 512, 'num_layers': 2}. Best is trial 0 with value: 53.985243659366795.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.020846435489753883\n"," \n","SMAPE : 134.8104958175032\n","\n"," Epoch 0 \t Training Loss: 0.05431412913215657 \t Validation Loss: 0.462807834148407 \n","\n","Validation Loss Decreased(inf--->0.462808) \t Saving The Model\n","Validation Loss Decreased(0.462808--->0.418779) \t Saving The Model\n","Validation Loss Decreased(0.418779--->0.380064) \t Saving The Model\n","Validation Loss Decreased(0.380064--->0.346184) \t Saving The Model\n","Validation Loss Decreased(0.346184--->0.317042) \t Saving The Model\n","Validation Loss Decreased(0.317042--->0.292754) \t Saving The Model\n","Validation Loss Decreased(0.292754--->0.273468) \t Saving The Model\n","Validation Loss Decreased(0.273468--->0.259147) \t Saving The Model\n","Validation Loss Decreased(0.259147--->0.249416) \t Saving The Model\n","Validation Loss Decreased(0.249416--->0.243558) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.029239083640277386 \t Validation Loss: 0.24063073098659515 \n","\n","Validation Loss Decreased(0.243558--->0.240631) \t Saving The Model\n","Validation Loss Decreased(0.240631--->0.239632) \t Saving The Model\n","Validation Loss Decreased(0.239632--->0.238820) \t Saving The Model\n","Validation Loss Decreased(0.238820--->0.236871) \t Saving The Model\n","Validation Loss Decreased(0.236871--->0.233731) \t Saving The Model\n","Validation Loss Decreased(0.233731--->0.229336) \t Saving The Model\n","Validation Loss Decreased(0.229336--->0.223686) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.026801307996114094 \t Validation Loss: 0.21681825816631317 \n","\n","Validation Loss Decreased(0.223686--->0.216818) \t Saving The Model\n","Validation Loss Decreased(0.216818--->0.208794) \t Saving The Model\n","Validation Loss Decreased(0.208794--->0.199685) \t Saving The Model\n","Validation Loss Decreased(0.199685--->0.189558) \t Saving The Model\n","Validation Loss Decreased(0.189558--->0.178475) \t Saving The Model\n","Validation Loss Decreased(0.178475--->0.166493) \t Saving The Model\n","Validation Loss Decreased(0.166493--->0.153691) \t Saving The Model\n","Validation Loss Decreased(0.153691--->0.140253) \t Saving The Model\n","Validation Loss Decreased(0.140253--->0.126676) \t Saving The Model\n","Validation Loss Decreased(0.126676--->0.114162) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.02432389014090101 \t Validation Loss: 0.10477481782436371 \n","\n","Validation Loss Decreased(0.114162--->0.104775) \t Saving The Model\n","Validation Loss Decreased(0.104775--->0.100175) \t Saving The Model\n","Validation Loss Decreased(0.100175--->0.099874) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.02368857633943359 \t Validation Loss: 0.10629596561193466 \n","\n","\n"," Epoch 50 \t Training Loss: 0.023308134560162824 \t Validation Loss: 0.10590890049934387 \n","\n","\n"," Epoch 60 \t Training Loss: 0.022936263276884954 \t Validation Loss: 0.110294409096241 \n","\n","\n"," Epoch 70 \t Training Loss: 0.02261317156565686 \t Validation Loss: 0.11701537668704987 \n","\n","\n"," Epoch 80 \t Training Loss: 0.022478982883815963 \t Validation Loss: 0.12641315162181854 \n","\n","\n"," Epoch 90 \t Training Loss: 0.022447325677300494 \t Validation Loss: 0.12906675040721893 \n","\n","\n"," Epoch 100 \t Training Loss: 0.022417269336680572 \t Validation Loss: 0.1292448341846466 \n","\n","\n"," Epoch 110 \t Training Loss: 0.0223950925283134 \t Validation Loss: 0.12999185919761658 \n","\n","\n"," Epoch 120 \t Training Loss: 0.02237680632000168 \t Validation Loss: 0.1306777000427246 \n","\n","\n"," Epoch 130 \t Training Loss: 0.022360936583330233 \t Validation Loss: 0.13130415976047516 \n","\n","\n"," Epoch 140 \t Training Loss: 0.022346798951427143 \t Validation Loss: 0.1319061666727066 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:32:01,420]\u001b[0m Trial 2 finished with value: 56.639284584575734 and parameters: {'batch_size': 512, 'learning_rate': 0.001538142642791941, 'hidden_size': 32, 'num_layers': 1}. Best is trial 0 with value: 53.985243659366795.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.09987391531467438\n"," \n","SMAPE : 56.639284584575734\n","\n"," Epoch 0 \t Training Loss: 2.329072663933039 \t Validation Loss: 0.2859949886798859 \n","\n","Validation Loss Decreased(inf--->0.285995) \t Saving The Model\n","Validation Loss Decreased(0.285995--->0.208939) \t Saving The Model\n","Validation Loss Decreased(0.208939--->0.147520) \t Saving The Model\n","Validation Loss Decreased(0.147520--->0.125650) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.0878112624399364 \t Validation Loss: 0.12761572003364563 \n","\n","Validation Loss Decreased(0.125650--->0.123898) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.043256534007377924 \t Validation Loss: 0.13821484595537187 \n","\n","\n"," Epoch 30 \t Training Loss: 0.07116699574980885 \t Validation Loss: 0.1286941200494766 \n","\n","\n"," Epoch 40 \t Training Loss: 0.04815026908181608 \t Validation Loss: 0.1369306318461895 \n","\n","\n"," Epoch 50 \t Training Loss: 0.06359099757391959 \t Validation Loss: 0.130972059071064 \n","\n","\n"," Epoch 60 \t Training Loss: 0.07521135578863322 \t Validation Loss: 0.1274818569421768 \n","\n","\n"," Epoch 70 \t Training Loss: 0.08084698915481567 \t Validation Loss: 0.1266436606645584 \n","\n","\n"," Epoch 80 \t Training Loss: 0.05621852235635742 \t Validation Loss: 0.13507519364356996 \n","\n","\n"," Epoch 90 \t Training Loss: 0.04249789817258716 \t Validation Loss: 0.1327170766890049 \n","\n","\n"," Epoch 100 \t Training Loss: 0.0371111900836695 \t Validation Loss: 0.1503640279173851 \n","\n","\n"," Epoch 110 \t Training Loss: 0.04100224206922576 \t Validation Loss: 0.16171834915876387 \n","\n","\n"," Epoch 120 \t Training Loss: 0.04679015218280256 \t Validation Loss: 0.17115668058395386 \n","\n","\n"," Epoch 130 \t Training Loss: 0.05066971087362617 \t Validation Loss: 0.17670146822929383 \n","\n","Validation Loss Decreased(0.123898--->0.117678) \t Saving The Model\n","Validation Loss Decreased(0.117678--->0.117192) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.03845885724877007 \t Validation Loss: 0.12579168677330016 \n","\n","Validation Loss Decreased(0.117192--->0.114793) \t Saving The Model\n","Validation Loss Decreased(0.114793--->0.114507) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:32:35,402]\u001b[0m Trial 3 finished with value: 44.28213036626703 and parameters: {'batch_size': 64, 'learning_rate': 0.18436611913419163, 'hidden_size': 16, 'num_layers': 3}. Best is trial 3 with value: 44.28213036626703.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.1145072340965271\n"," \n","SMAPE : 44.28213036626703\n","\n"," Epoch 0 \t Training Loss: 1.9108835741877557 \t Validation Loss: 0.7877604067325592 \n","\n","Validation Loss Decreased(inf--->0.787760) \t Saving The Model\n","Validation Loss Decreased(0.787760--->0.305057) \t Saving The Model\n","Validation Loss Decreased(0.305057--->0.254759) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.032653094409033655 \t Validation Loss: 0.2888445109128952 \n","\n","Validation Loss Decreased(0.254759--->0.252985) \t Saving The Model\n","Validation Loss Decreased(0.252985--->0.252437) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.03235280951485038 \t Validation Loss: 0.25382745265960693 \n","\n","Validation Loss Decreased(0.252437--->0.248897) \t Saving The Model\n","Validation Loss Decreased(0.248897--->0.246712) \t Saving The Model\n","Validation Loss Decreased(0.246712--->0.240823) \t Saving The Model\n","Validation Loss Decreased(0.240823--->0.238233) \t Saving The Model\n","Validation Loss Decreased(0.238233--->0.232177) \t Saving The Model\n","Validation Loss Decreased(0.232177--->0.227563) \t Saving The Model\n","Validation Loss Decreased(0.227563--->0.222571) \t Saving The Model\n","Validation Loss Decreased(0.222571--->0.218404) \t Saving The Model\n","Validation Loss Decreased(0.218404--->0.215601) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.029808660270646214 \t Validation Loss: 0.2138092964887619 \n","\n","Validation Loss Decreased(0.215601--->0.213809) \t Saving The Model\n","Validation Loss Decreased(0.213809--->0.213161) \t Saving The Model\n","Validation Loss Decreased(0.213161--->0.212954) \t Saving The Model\n","Validation Loss Decreased(0.212954--->0.212708) \t Saving The Model\n","Validation Loss Decreased(0.212708--->0.212228) \t Saving The Model\n","Validation Loss Decreased(0.212228--->0.212179) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.029704384249635042 \t Validation Loss: 0.21264150738716125 \n","\n","Validation Loss Decreased(0.212179--->0.210397) \t Saving The Model\n","Validation Loss Decreased(0.210397--->0.207864) \t Saving The Model\n","Validation Loss Decreased(0.207864--->0.206345) \t Saving The Model\n","Validation Loss Decreased(0.206345--->0.206099) \t Saving The Model\n","Validation Loss Decreased(0.206099--->0.205884) \t Saving The Model\n","Validation Loss Decreased(0.205884--->0.204229) \t Saving The Model\n","Validation Loss Decreased(0.204229--->0.201773) \t Saving The Model\n","Validation Loss Decreased(0.201773--->0.198756) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.027608916629105806 \t Validation Loss: 0.19411002099514008 \n","\n","Validation Loss Decreased(0.198756--->0.194110) \t Saving The Model\n","Validation Loss Decreased(0.194110--->0.187857) \t Saving The Model\n","Validation Loss Decreased(0.187857--->0.182360) \t Saving The Model\n","Validation Loss Decreased(0.182360--->0.178194) \t Saving The Model\n","Validation Loss Decreased(0.178194--->0.175409) \t Saving The Model\n","Validation Loss Decreased(0.175409--->0.171471) \t Saving The Model\n","Validation Loss Decreased(0.171471--->0.164743) \t Saving The Model\n","Validation Loss Decreased(0.164743--->0.156320) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.023840426048263908 \t Validation Loss: 0.15701128542423248 \n","\n","Validation Loss Decreased(0.156320--->0.154718) \t Saving The Model\n","Validation Loss Decreased(0.154718--->0.152906) \t Saving The Model\n","Validation Loss Decreased(0.152906--->0.127171) \t Saving The Model\n","Validation Loss Decreased(0.127171--->0.126549) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0284913151524961 \t Validation Loss: 0.1489536017179489 \n","\n","Validation Loss Decreased(0.126549--->0.124715) \t Saving The Model\n","Validation Loss Decreased(0.124715--->0.122124) \t Saving The Model\n","Validation Loss Decreased(0.122124--->0.110013) \t Saving The Model\n","Validation Loss Decreased(0.110013--->0.103576) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.01917790900915861 \t Validation Loss: 0.10070423781871796 \n","\n","Validation Loss Decreased(0.103576--->0.100704) \t Saving The Model\n","Validation Loss Decreased(0.100704--->0.077567) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.035967099480330944 \t Validation Loss: 0.15625325590372086 \n","\n","\n"," Epoch 100 \t Training Loss: 0.019054896337911485 \t Validation Loss: 0.1221180409193039 \n","\n","\n"," Epoch 110 \t Training Loss: 0.0267209037207067 \t Validation Loss: 0.14318278431892395 \n","\n","\n"," Epoch 120 \t Training Loss: 0.030178884416818617 \t Validation Loss: 0.0892324261367321 \n","\n","\n"," Epoch 130 \t Training Loss: 0.02353379763662815 \t Validation Loss: 0.13116345182061195 \n","\n","\n"," Epoch 140 \t Training Loss: 0.015905175218358637 \t Validation Loss: 0.10357758402824402 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:32:49,987]\u001b[0m Trial 4 finished with value: 69.3187233325463 and parameters: {'batch_size': 256, 'learning_rate': 0.08569386545957428, 'hidden_size': 32, 'num_layers': 4}. Best is trial 3 with value: 44.28213036626703.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.07756712473928928\n"," \n","SMAPE : 69.3187233325463\n","\n"," Epoch 0 \t Training Loss: 0.13128627526263395 \t Validation Loss: 0.46765604615211487 \n","\n","Validation Loss Decreased(inf--->0.467656) \t Saving The Model\n","Validation Loss Decreased(0.467656--->0.334699) \t Saving The Model\n","Validation Loss Decreased(0.334699--->0.166080) \t Saving The Model\n","Validation Loss Decreased(0.166080--->0.132164) \t Saving The Model\n","Validation Loss Decreased(0.132164--->0.130488) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.024829720535005134 \t Validation Loss: 0.10569744557142258 \n","\n","Validation Loss Decreased(0.130488--->0.105697) \t Saving The Model\n","Validation Loss Decreased(0.105697--->0.081770) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.02301029992910723 \t Validation Loss: 0.1288297325372696 \n","\n","\n"," Epoch 30 \t Training Loss: 0.022702052257955074 \t Validation Loss: 0.14013724029064178 \n","\n","\n"," Epoch 40 \t Training Loss: 0.02256654854863882 \t Validation Loss: 0.144472137093544 \n","\n","\n"," Epoch 50 \t Training Loss: 0.022503975468377273 \t Validation Loss: 0.14592796564102173 \n","\n","\n"," Epoch 60 \t Training Loss: 0.02245704286421339 \t Validation Loss: 0.14691893756389618 \n","\n","\n"," Epoch 70 \t Training Loss: 0.022417681602140267 \t Validation Loss: 0.14749707281589508 \n","\n","\n"," Epoch 80 \t Training Loss: 0.022384412509078782 \t Validation Loss: 0.14795222878456116 \n","\n","\n"," Epoch 90 \t Training Loss: 0.022355268942192197 \t Validation Loss: 0.14831598103046417 \n","\n","\n"," Epoch 100 \t Training Loss: 0.02232911701624592 \t Validation Loss: 0.1486394852399826 \n","\n","\n"," Epoch 110 \t Training Loss: 0.022305092153449852 \t Validation Loss: 0.1490165889263153 \n","\n","\n"," Epoch 120 \t Training Loss: 0.022282490817209084 \t Validation Loss: 0.1494533121585846 \n","\n","\n"," Epoch 130 \t Training Loss: 0.022260600080092747 \t Validation Loss: 0.14970676600933075 \n","\n","\n"," Epoch 140 \t Training Loss: 0.022238485282287 \t Validation Loss: 0.14924277365207672 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:32:56,300]\u001b[0m Trial 5 finished with value: 40.28238062952473 and parameters: {'batch_size': 512, 'learning_rate': 0.03002857946268959, 'hidden_size': 32, 'num_layers': 2}. Best is trial 5 with value: 40.28238062952473.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.08177049458026886\n"," \n","SMAPE : 40.28238062952473\n","\n"," Epoch 0 \t Training Loss: 0.030128746554692044 \t Validation Loss: 0.43357575237751006 \n","\n","Validation Loss Decreased(inf--->0.433576) \t Saving The Model\n","Validation Loss Decreased(0.433576--->0.347815) \t Saving The Model\n","Validation Loss Decreased(0.347815--->0.333886) \t Saving The Model\n","Validation Loss Decreased(0.333886--->0.329617) \t Saving The Model\n","Validation Loss Decreased(0.329617--->0.317308) \t Saving The Model\n","Validation Loss Decreased(0.317308--->0.297482) \t Saving The Model\n","Validation Loss Decreased(0.297482--->0.264891) \t Saving The Model\n","Validation Loss Decreased(0.264891--->0.198825) \t Saving The Model\n","Validation Loss Decreased(0.198825--->0.083507) \t Saving The Model\n","Validation Loss Decreased(0.083507--->0.020966) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.005677568264945876 \t Validation Loss: 0.02218860909342766 \n","\n","Validation Loss Decreased(0.020966--->0.014990) \t Saving The Model\n","Validation Loss Decreased(0.014990--->0.013787) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.002651610686643835 \t Validation Loss: 0.01755395196378231 \n","\n","\n"," Epoch 30 \t Training Loss: 0.0027392409369895176 \t Validation Loss: 0.01800406174734235 \n","\n","\n"," Epoch 40 \t Training Loss: 0.0027349790891094015 \t Validation Loss: 0.0194436427205801 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0027287020340281743 \t Validation Loss: 0.020377442054450513 \n","\n","\n"," Epoch 60 \t Training Loss: 0.002629100253670913 \t Validation Loss: 0.01965167298913002 \n","\n","\n"," Epoch 70 \t Training Loss: 0.002800416880290868 \t Validation Loss: 0.018453215900808572 \n","\n","\n"," Epoch 80 \t Training Loss: 0.0030966258340413334 \t Validation Loss: 0.017812861781567334 \n","\n","\n"," Epoch 90 \t Training Loss: 0.0029887598416280525 \t Validation Loss: 0.01857254095375538 \n","\n","\n"," Epoch 100 \t Training Loss: 0.0029787307948026865 \t Validation Loss: 0.019181072106584908 \n","\n","\n"," Epoch 110 \t Training Loss: 0.00294314675802525 \t Validation Loss: 0.01987599767744541 \n","\n","\n"," Epoch 120 \t Training Loss: 0.0029188540191171343 \t Validation Loss: 0.020563448686152695 \n","\n","\n"," Epoch 130 \t Training Loss: 0.0028866899809145254 \t Validation Loss: 0.021322921104729175 \n","\n","\n"," Epoch 140 \t Training Loss: 0.0028560132479469756 \t Validation Loss: 0.022114336490631104 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:33:21,306]\u001b[0m Trial 6 finished with value: 48.51537046015514 and parameters: {'batch_size': 64, 'learning_rate': 0.001816582153753775, 'hidden_size': 16, 'num_layers': 2}. Best is trial 5 with value: 40.28238062952473.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.013786817248910666\n"," \n","SMAPE : 48.51537046015514\n","\n"," Epoch 0 \t Training Loss: 0.022796441253740342 \t Validation Loss: 0.18827576115727424 \n","\n","Validation Loss Decreased(inf--->0.188276) \t Saving The Model\n","Validation Loss Decreased(0.188276--->0.158746) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.028290396140073427 \t Validation Loss: 0.37542605102062226 \n","\n","\n"," Epoch 20 \t Training Loss: 0.0112752937246114 \t Validation Loss: 0.1007685698568821 \n","\n","Validation Loss Decreased(0.158746--->0.100769) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.022714841258130038 \t Validation Loss: 0.3167910575866699 \n","\n","\n"," Epoch 40 \t Training Loss: 0.012788764212746172 \t Validation Loss: 0.1736393839120865 \n","\n","Validation Loss Decreased(0.100769--->0.099226) \t Saving The Model\n","Validation Loss Decreased(0.099226--->0.096381) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.012196027746540494 \t Validation Loss: 0.15084692537784578 \n","\n","Validation Loss Decreased(0.096381--->0.052638) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0068195171421393756 \t Validation Loss: 0.07072418108582497 \n","\n","Validation Loss Decreased(0.052638--->0.050540) \t Saving The Model\n","Validation Loss Decreased(0.050540--->0.025829) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0037355768341512884 \t Validation Loss: 0.025130067579448225 \n","\n","Validation Loss Decreased(0.025829--->0.025130) \t Saving The Model\n","Validation Loss Decreased(0.025130--->0.024111) \t Saving The Model\n","Validation Loss Decreased(0.024111--->0.022873) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0045904308521130584 \t Validation Loss: 0.023237980995327236 \n","\n","Validation Loss Decreased(0.022873--->0.022678) \t Saving The Model\n","Validation Loss Decreased(0.022678--->0.022629) \t Saving The Model\n","Validation Loss Decreased(0.022629--->0.022236) \t Saving The Model\n","Validation Loss Decreased(0.022236--->0.022115) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.004899038246367127 \t Validation Loss: 0.021809390000998973 \n","\n","Validation Loss Decreased(0.022115--->0.021809) \t Saving The Model\n","Validation Loss Decreased(0.021809--->0.021734) \t Saving The Model\n","Validation Loss Decreased(0.021734--->0.021557) \t Saving The Model\n","Validation Loss Decreased(0.021557--->0.021537) \t Saving The Model\n","Validation Loss Decreased(0.021537--->0.021439) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.004733206392847933 \t Validation Loss: 0.021432840917259454 \n","\n","Validation Loss Decreased(0.021439--->0.021433) \t Saving The Model\n","Validation Loss Decreased(0.021433--->0.021375) \t Saving The Model\n","Validation Loss Decreased(0.021375--->0.021369) \t Saving The Model\n","Validation Loss Decreased(0.021369--->0.021334) \t Saving The Model\n","Validation Loss Decreased(0.021334--->0.021327) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.004610620153835043 \t Validation Loss: 0.02130594998598099 \n","\n","Validation Loss Decreased(0.021327--->0.021306) \t Saving The Model\n","Validation Loss Decreased(0.021306--->0.021299) \t Saving The Model\n","Validation Loss Decreased(0.021299--->0.021287) \t Saving The Model\n","Validation Loss Decreased(0.021287--->0.021283) \t Saving The Model\n","Validation Loss Decreased(0.021283--->0.021277) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.004602280168910511 \t Validation Loss: 0.0212747598066926 \n","\n","Validation Loss Decreased(0.021277--->0.021275) \t Saving The Model\n","Validation Loss Decreased(0.021275--->0.021273) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.00461675221886253 \t Validation Loss: 0.021278703957796095 \n","\n","\n"," Epoch 140 \t Training Loss: 0.00463121373904869 \t Validation Loss: 0.02129724845290184 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:34:04,136]\u001b[0m Trial 7 finished with value: 25.80798653882269 and parameters: {'batch_size': 64, 'learning_rate': 0.014949675790007754, 'hidden_size': 16, 'num_layers': 4}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.021273000072687863\n"," \n","SMAPE : 25.80798653882269\n","\n"," Epoch 0 \t Training Loss: 0.022631356632336973 \t Validation Loss: 0.34215114514033 \n","\n","Validation Loss Decreased(inf--->0.342151) \t Saving The Model\n","Validation Loss Decreased(0.342151--->0.341872) \t Saving The Model\n","Validation Loss Decreased(0.341872--->0.341490) \t Saving The Model\n","Validation Loss Decreased(0.341490--->0.341097) \t Saving The Model\n","Validation Loss Decreased(0.341097--->0.340712) \t Saving The Model\n","Validation Loss Decreased(0.340712--->0.340342) \t Saving The Model\n","Validation Loss Decreased(0.340342--->0.339984) \t Saving The Model\n","Validation Loss Decreased(0.339984--->0.339637) \t Saving The Model\n","Validation Loss Decreased(0.339637--->0.339299) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.021748753800056874 \t Validation Loss: 0.33896783987681073 \n","\n","Validation Loss Decreased(0.339299--->0.338968) \t Saving The Model\n","Validation Loss Decreased(0.338968--->0.338641) \t Saving The Model\n","Validation Loss Decreased(0.338641--->0.338317) \t Saving The Model\n","Validation Loss Decreased(0.338317--->0.337994) \t Saving The Model\n","Validation Loss Decreased(0.337994--->0.337673) \t Saving The Model\n","Validation Loss Decreased(0.337673--->0.337351) \t Saving The Model\n","Validation Loss Decreased(0.337351--->0.337028) \t Saving The Model\n","Validation Loss Decreased(0.337028--->0.336704) \t Saving The Model\n","Validation Loss Decreased(0.336704--->0.336377) \t Saving The Model\n","Validation Loss Decreased(0.336377--->0.336048) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.021464418433606625 \t Validation Loss: 0.3357165952523549 \n","\n","Validation Loss Decreased(0.336048--->0.335717) \t Saving The Model\n","Validation Loss Decreased(0.335717--->0.335382) \t Saving The Model\n","Validation Loss Decreased(0.335382--->0.335043) \t Saving The Model\n","Validation Loss Decreased(0.335043--->0.334701) \t Saving The Model\n","Validation Loss Decreased(0.334701--->0.334354) \t Saving The Model\n","Validation Loss Decreased(0.334354--->0.334004) \t Saving The Model\n","Validation Loss Decreased(0.334004--->0.333649) \t Saving The Model\n","Validation Loss Decreased(0.333649--->0.333289) \t Saving The Model\n","Validation Loss Decreased(0.333289--->0.332925) \t Saving The Model\n","Validation Loss Decreased(0.332925--->0.332556) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.021226054267026485 \t Validation Loss: 0.3321816325187683 \n","\n","Validation Loss Decreased(0.332556--->0.332182) \t Saving The Model\n","Validation Loss Decreased(0.332182--->0.331802) \t Saving The Model\n","Validation Loss Decreased(0.331802--->0.331418) \t Saving The Model\n","Validation Loss Decreased(0.331418--->0.331028) \t Saving The Model\n","Validation Loss Decreased(0.331028--->0.330632) \t Saving The Model\n","Validation Loss Decreased(0.330632--->0.330231) \t Saving The Model\n","Validation Loss Decreased(0.330231--->0.329823) \t Saving The Model\n","Validation Loss Decreased(0.329823--->0.329410) \t Saving The Model\n","Validation Loss Decreased(0.329410--->0.328991) \t Saving The Model\n","Validation Loss Decreased(0.328991--->0.328565) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.020973381912335755 \t Validation Loss: 0.32813285787900287 \n","\n","Validation Loss Decreased(0.328565--->0.328133) \t Saving The Model\n","Validation Loss Decreased(0.328133--->0.327694) \t Saving The Model\n","Validation Loss Decreased(0.327694--->0.327248) \t Saving The Model\n","Validation Loss Decreased(0.327248--->0.326795) \t Saving The Model\n","Validation Loss Decreased(0.326795--->0.326335) \t Saving The Model\n","Validation Loss Decreased(0.326335--->0.325867) \t Saving The Model\n","Validation Loss Decreased(0.325867--->0.325392) \t Saving The Model\n","Validation Loss Decreased(0.325392--->0.324908) \t Saving The Model\n","Validation Loss Decreased(0.324908--->0.324416) \t Saving The Model\n","Validation Loss Decreased(0.324416--->0.323915) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.020686939847655593 \t Validation Loss: 0.3234056333700816 \n","\n","Validation Loss Decreased(0.323915--->0.323406) \t Saving The Model\n","Validation Loss Decreased(0.323406--->0.322887) \t Saving The Model\n","Validation Loss Decreased(0.322887--->0.322358) \t Saving The Model\n","Validation Loss Decreased(0.322358--->0.321819) \t Saving The Model\n","Validation Loss Decreased(0.321819--->0.321270) \t Saving The Model\n","Validation Loss Decreased(0.321270--->0.320710) \t Saving The Model\n","Validation Loss Decreased(0.320710--->0.320139) \t Saving The Model\n","Validation Loss Decreased(0.320139--->0.319555) \t Saving The Model\n","Validation Loss Decreased(0.319555--->0.318960) \t Saving The Model\n","Validation Loss Decreased(0.318960--->0.318351) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.020346147101372482 \t Validation Loss: 0.3177292545636495 \n","\n","Validation Loss Decreased(0.318351--->0.317729) \t Saving The Model\n","Validation Loss Decreased(0.317729--->0.317093) \t Saving The Model\n","Validation Loss Decreased(0.317093--->0.316443) \t Saving The Model\n","Validation Loss Decreased(0.316443--->0.315777) \t Saving The Model\n","Validation Loss Decreased(0.315777--->0.315095) \t Saving The Model\n","Validation Loss Decreased(0.315095--->0.314396) \t Saving The Model\n","Validation Loss Decreased(0.314396--->0.313680) \t Saving The Model\n","Validation Loss Decreased(0.313680--->0.312946) \t Saving The Model\n","Validation Loss Decreased(0.312946--->0.312193) \t Saving The Model\n","Validation Loss Decreased(0.312193--->0.311420) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.019919191987719388 \t Validation Loss: 0.3106263279914856 \n","\n","Validation Loss Decreased(0.311420--->0.310626) \t Saving The Model\n","Validation Loss Decreased(0.310626--->0.309811) \t Saving The Model\n","Validation Loss Decreased(0.309811--->0.308973) \t Saving The Model\n","Validation Loss Decreased(0.308973--->0.308111) \t Saving The Model\n","Validation Loss Decreased(0.308111--->0.307224) \t Saving The Model\n","Validation Loss Decreased(0.307224--->0.306312) \t Saving The Model\n","Validation Loss Decreased(0.306312--->0.305372) \t Saving The Model\n","Validation Loss Decreased(0.305372--->0.304404) \t Saving The Model\n","Validation Loss Decreased(0.304404--->0.303406) \t Saving The Model\n","Validation Loss Decreased(0.303406--->0.302377) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.019354741973802447 \t Validation Loss: 0.30131539702415466 \n","\n","Validation Loss Decreased(0.302377--->0.301315) \t Saving The Model\n","Validation Loss Decreased(0.301315--->0.300220) \t Saving The Model\n","Validation Loss Decreased(0.300220--->0.299088) \t Saving The Model\n","Validation Loss Decreased(0.299088--->0.297919) \t Saving The Model\n","Validation Loss Decreased(0.297919--->0.296710) \t Saving The Model\n","Validation Loss Decreased(0.296710--->0.295460) \t Saving The Model\n","Validation Loss Decreased(0.295460--->0.294166) \t Saving The Model\n","Validation Loss Decreased(0.294166--->0.292827) \t Saving The Model\n","Validation Loss Decreased(0.292827--->0.291439) \t Saving The Model\n","Validation Loss Decreased(0.291439--->0.290001) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.01856469934573397 \t Validation Loss: 0.28850947817166644 \n","\n","Validation Loss Decreased(0.290001--->0.288509) \t Saving The Model\n","Validation Loss Decreased(0.288509--->0.286962) \t Saving The Model\n","Validation Loss Decreased(0.286962--->0.285355) \t Saving The Model\n","Validation Loss Decreased(0.285355--->0.283685) \t Saving The Model\n","Validation Loss Decreased(0.283685--->0.281949) \t Saving The Model\n","Validation Loss Decreased(0.281949--->0.280144) \t Saving The Model\n","Validation Loss Decreased(0.280144--->0.278264) \t Saving The Model\n","Validation Loss Decreased(0.278264--->0.276306) \t Saving The Model\n","Validation Loss Decreased(0.276306--->0.274265) \t Saving The Model\n","Validation Loss Decreased(0.274265--->0.272136) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.017382116318913177 \t Validation Loss: 0.2699130078156789 \n","\n","Validation Loss Decreased(0.272136--->0.269913) \t Saving The Model\n","Validation Loss Decreased(0.269913--->0.267591) \t Saving The Model\n","Validation Loss Decreased(0.267591--->0.265163) \t Saving The Model\n","Validation Loss Decreased(0.265163--->0.262622) \t Saving The Model\n","Validation Loss Decreased(0.262622--->0.259962) \t Saving The Model\n","Validation Loss Decreased(0.259962--->0.257173) \t Saving The Model\n","Validation Loss Decreased(0.257173--->0.254249) \t Saving The Model\n","Validation Loss Decreased(0.254249--->0.251180) \t Saving The Model\n","Validation Loss Decreased(0.251180--->0.247957) \t Saving The Model\n","Validation Loss Decreased(0.247957--->0.244570) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.015465057012625039 \t Validation Loss: 0.24100851515928903 \n","\n","Validation Loss Decreased(0.244570--->0.241009) \t Saving The Model\n","Validation Loss Decreased(0.241009--->0.237262) \t Saving The Model\n","Validation Loss Decreased(0.237262--->0.233321) \t Saving The Model\n","Validation Loss Decreased(0.233321--->0.229174) \t Saving The Model\n","Validation Loss Decreased(0.229174--->0.224812) \t Saving The Model\n","Validation Loss Decreased(0.224812--->0.220227) \t Saving The Model\n","Validation Loss Decreased(0.220227--->0.215414) \t Saving The Model\n","Validation Loss Decreased(0.215414--->0.210371) \t Saving The Model\n","Validation Loss Decreased(0.210371--->0.205101) \t Saving The Model\n","Validation Loss Decreased(0.205101--->0.199613) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.012212620297214016 \t Validation Loss: 0.19392631451288858 \n","\n","Validation Loss Decreased(0.199613--->0.193926) \t Saving The Model\n","Validation Loss Decreased(0.193926--->0.188067) \t Saving The Model\n","Validation Loss Decreased(0.188067--->0.182071) \t Saving The Model\n","Validation Loss Decreased(0.182071--->0.175984) \t Saving The Model\n","Validation Loss Decreased(0.175984--->0.169857) \t Saving The Model\n","Validation Loss Decreased(0.169857--->0.163743) \t Saving The Model\n","Validation Loss Decreased(0.163743--->0.157698) \t Saving The Model\n","Validation Loss Decreased(0.157698--->0.151779) \t Saving The Model\n","Validation Loss Decreased(0.151779--->0.146039) \t Saving The Model\n","Validation Loss Decreased(0.146039--->0.140531) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.00833489883079892 \t Validation Loss: 0.13529602934916815 \n","\n","Validation Loss Decreased(0.140531--->0.135296) \t Saving The Model\n","Validation Loss Decreased(0.135296--->0.130369) \t Saving The Model\n","Validation Loss Decreased(0.130369--->0.125771) \t Saving The Model\n","Validation Loss Decreased(0.125771--->0.121510) \t Saving The Model\n","Validation Loss Decreased(0.121510--->0.117577) \t Saving The Model\n","Validation Loss Decreased(0.117577--->0.113954) \t Saving The Model\n","Validation Loss Decreased(0.113954--->0.110613) \t Saving The Model\n","Validation Loss Decreased(0.110613--->0.107521) \t Saving The Model\n","Validation Loss Decreased(0.107521--->0.104644) \t Saving The Model\n","Validation Loss Decreased(0.104644--->0.101953) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.00691623113816604 \t Validation Loss: 0.09941772371530533 \n","\n","Validation Loss Decreased(0.101953--->0.099418) \t Saving The Model\n","Validation Loss Decreased(0.099418--->0.097018) \t Saving The Model\n","Validation Loss Decreased(0.097018--->0.094734) \t Saving The Model\n","Validation Loss Decreased(0.094734--->0.092552) \t Saving The Model\n","Validation Loss Decreased(0.092552--->0.090461) \t Saving The Model\n","Validation Loss Decreased(0.090461--->0.088453) \t Saving The Model\n","Validation Loss Decreased(0.088453--->0.086520) \t Saving The Model\n","Validation Loss Decreased(0.086520--->0.084658) \t Saving The Model\n","Validation Loss Decreased(0.084658--->0.082862) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:34:18,510]\u001b[0m Trial 8 finished with value: 46.45847268522542 and parameters: {'batch_size': 128, 'learning_rate': 0.00010360136475441584, 'hidden_size': 16, 'num_layers': 2}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.082862--->0.081130) \t Saving The Model\n","best loss for the trial =  0.08113034442067146\n"," \n","SMAPE : 46.45847268522542\n","\n"," Epoch 0 \t Training Loss: 0.058354336163029076 \t Validation Loss: 0.4161609709262848 \n","\n","Validation Loss Decreased(inf--->0.416161) \t Saving The Model\n","Validation Loss Decreased(0.416161--->0.354062) \t Saving The Model\n","Validation Loss Decreased(0.354062--->0.304043) \t Saving The Model\n","Validation Loss Decreased(0.304043--->0.277761) \t Saving The Model\n","Validation Loss Decreased(0.277761--->0.263390) \t Saving The Model\n","Validation Loss Decreased(0.263390--->0.214735) \t Saving The Model\n","Validation Loss Decreased(0.214735--->0.212414) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.0192186183296144 \t Validation Loss: 0.1363205760717392 \n","\n","Validation Loss Decreased(0.212414--->0.136321) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.018330240435898305 \t Validation Loss: 0.12453978881239891 \n","\n","Validation Loss Decreased(0.136321--->0.124540) \t Saving The Model\n","Validation Loss Decreased(0.124540--->0.100713) \t Saving The Model\n","Validation Loss Decreased(0.100713--->0.096818) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.013941503770183772 \t Validation Loss: 0.11660443991422653 \n","\n","\n"," Epoch 40 \t Training Loss: 0.013663313759025186 \t Validation Loss: 0.11592698842287064 \n","\n","\n"," Epoch 50 \t Training Loss: 0.013556834706105293 \t Validation Loss: 0.11745912581682205 \n","\n","\n"," Epoch 60 \t Training Loss: 0.013497854408342391 \t Validation Loss: 0.11846470460295677 \n","\n","\n"," Epoch 70 \t Training Loss: 0.013453923992346972 \t Validation Loss: 0.11908203735947609 \n","\n","\n"," Epoch 80 \t Training Loss: 0.013418527343310416 \t Validation Loss: 0.11952004581689835 \n","\n","\n"," Epoch 90 \t Training Loss: 0.01338794237235561 \t Validation Loss: 0.1198253445327282 \n","\n","\n"," Epoch 100 \t Training Loss: 0.013360763667151332 \t Validation Loss: 0.12005044892430305 \n","\n","\n"," Epoch 110 \t Training Loss: 0.013335931667825207 \t Validation Loss: 0.12021763622760773 \n","\n","\n"," Epoch 120 \t Training Loss: 0.013312636129558086 \t Validation Loss: 0.12033558636903763 \n","\n","\n"," Epoch 130 \t Training Loss: 0.013290111068636179 \t Validation Loss: 0.12040824443101883 \n","\n","\n"," Epoch 140 \t Training Loss: 0.01326736029004678 \t Validation Loss: 0.12045775353908539 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:34:32,856]\u001b[0m Trial 9 finished with value: 41.48149423173369 and parameters: {'batch_size': 256, 'learning_rate': 0.013621827640845555, 'hidden_size': 64, 'num_layers': 4}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.09681778773665428\n"," \n","SMAPE : 41.48149423173369\n","\n"," Epoch 0 \t Training Loss: 1.70833162444178 \t Validation Loss: 0.05814031511545181 \n","\n","Validation Loss Decreased(inf--->0.058140) \t Saving The Model\n","Validation Loss Decreased(0.058140--->0.053205) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.06894145463593304 \t Validation Loss: 0.08632310032844544 \n","\n","\n"," Epoch 20 \t Training Loss: 0.10224729385226965 \t Validation Loss: 0.1759742707014084 \n","\n","\n"," Epoch 30 \t Training Loss: 0.06641229474917054 \t Validation Loss: 0.15602035522460939 \n","\n","\n"," Epoch 40 \t Training Loss: 0.0756834927946329 \t Validation Loss: 0.17356407940387725 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0637370691052638 \t Validation Loss: 0.16525356322526932 \n","\n","\n"," Epoch 60 \t Training Loss: 0.05707815741188824 \t Validation Loss: 0.1685513362288475 \n","\n","Validation Loss Decreased(0.053205--->0.038899) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0793668873840943 \t Validation Loss: 0.2521394550800323 \n","\n","\n"," Epoch 80 \t Training Loss: 0.04548696707934141 \t Validation Loss: 0.21076648831367492 \n","\n","\n"," Epoch 90 \t Training Loss: 0.03299569491064176 \t Validation Loss: 0.08289670944213867 \n","\n","\n"," Epoch 100 \t Training Loss: 0.03498686735983938 \t Validation Loss: 0.06719016432762145 \n","\n","\n"," Epoch 110 \t Training Loss: 0.04433541977778077 \t Validation Loss: 0.07392399050295353 \n","\n","\n"," Epoch 120 \t Training Loss: 0.027289555029710755 \t Validation Loss: 0.09613977409899235 \n","\n","\n"," Epoch 130 \t Training Loss: 0.01682739957468584 \t Validation Loss: 0.07840942181646823 \n","\n","Validation Loss Decreased(0.038899--->0.034281) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.03339536767452955 \t Validation Loss: 0.06584702134132385 \n","\n","best loss for the trial =  0.03428102638572454\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:46:19,256]\u001b[0m Trial 10 finished with value: 92.47296389488537 and parameters: {'batch_size': 64, 'learning_rate': 0.0024965410942844155, 'hidden_size': 1024, 'num_layers': 5}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":[" \n","SMAPE : 92.47296389488537\n","\n"," Epoch 0 \t Training Loss: 27.051092917410035 \t Validation Loss: 37.1747932434082 \n","\n","Validation Loss Decreased(inf--->37.174793) \t Saving The Model\n","Validation Loss Decreased(37.174793--->8.485135) \t Saving The Model\n","Validation Loss Decreased(8.485135--->2.808505) \t Saving The Model\n","Validation Loss Decreased(2.808505--->1.224120) \t Saving The Model\n","Validation Loss Decreased(1.224120--->0.677276) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.5433385173479716 \t Validation Loss: 1.111399531364441 \n","\n","Validation Loss Decreased(0.677276--->0.276251) \t Saving The Model\n","Validation Loss Decreased(0.276251--->0.252213) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.10954370225469272 \t Validation Loss: 0.3680953085422516 \n","\n","\n"," Epoch 30 \t Training Loss: 0.09410991271336873 \t Validation Loss: 0.2779178023338318 \n","\n","Validation Loss Decreased(0.252213--->0.251996) \t Saving The Model\n","Validation Loss Decreased(0.251996--->0.246567) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.11472473541895549 \t Validation Loss: 0.24492856860160828 \n","\n","Validation Loss Decreased(0.246567--->0.244929) \t Saving The Model\n","Validation Loss Decreased(0.244929--->0.239048) \t Saving The Model\n","Validation Loss Decreased(0.239048--->0.236089) \t Saving The Model\n","Validation Loss Decreased(0.236089--->0.235631) \t Saving The Model\n","Validation Loss Decreased(0.235631--->0.229739) \t Saving The Model\n","Validation Loss Decreased(0.229739--->0.225706) \t Saving The Model\n","Validation Loss Decreased(0.225706--->0.224660) \t Saving The Model\n","Validation Loss Decreased(0.224660--->0.220298) \t Saving The Model\n","Validation Loss Decreased(0.220298--->0.216200) \t Saving The Model\n","Validation Loss Decreased(0.216200--->0.213118) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.14906391501426697 \t Validation Loss: 0.21035659313201904 \n","\n","Validation Loss Decreased(0.213118--->0.210357) \t Saving The Model\n","Validation Loss Decreased(0.210357--->0.206575) \t Saving The Model\n","Validation Loss Decreased(0.206575--->0.202558) \t Saving The Model\n","Validation Loss Decreased(0.202558--->0.199608) \t Saving The Model\n","Validation Loss Decreased(0.199608--->0.196496) \t Saving The Model\n","Validation Loss Decreased(0.196496--->0.192709) \t Saving The Model\n","Validation Loss Decreased(0.192709--->0.189140) \t Saving The Model\n","Validation Loss Decreased(0.189140--->0.186002) \t Saving The Model\n","Validation Loss Decreased(0.186002--->0.182689) \t Saving The Model\n","Validation Loss Decreased(0.182689--->0.179314) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.1906679222981135 \t Validation Loss: 0.17580853402614594 \n","\n","Validation Loss Decreased(0.179314--->0.175809) \t Saving The Model\n","Validation Loss Decreased(0.175809--->0.172580) \t Saving The Model\n","Validation Loss Decreased(0.172580--->0.169448) \t Saving The Model\n","Validation Loss Decreased(0.169448--->0.166190) \t Saving The Model\n","Validation Loss Decreased(0.166190--->0.162904) \t Saving The Model\n","Validation Loss Decreased(0.162904--->0.159773) \t Saving The Model\n","Validation Loss Decreased(0.159773--->0.156710) \t Saving The Model\n","Validation Loss Decreased(0.156710--->0.153678) \t Saving The Model\n","Validation Loss Decreased(0.153678--->0.150663) \t Saving The Model\n","Validation Loss Decreased(0.150663--->0.147670) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.23827750980854034 \t Validation Loss: 0.1447792947292328 \n","\n","Validation Loss Decreased(0.147670--->0.144779) \t Saving The Model\n","Validation Loss Decreased(0.144779--->0.141974) \t Saving The Model\n","Validation Loss Decreased(0.141974--->0.139204) \t Saving The Model\n","Validation Loss Decreased(0.139204--->0.136467) \t Saving The Model\n","Validation Loss Decreased(0.136467--->0.133810) \t Saving The Model\n","Validation Loss Decreased(0.133810--->0.131221) \t Saving The Model\n","Validation Loss Decreased(0.131221--->0.128709) \t Saving The Model\n","Validation Loss Decreased(0.128709--->0.126260) \t Saving The Model\n","Validation Loss Decreased(0.126260--->0.123871) \t Saving The Model\n","Validation Loss Decreased(0.123871--->0.121540) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.28625952700773877 \t Validation Loss: 0.11928936094045639 \n","\n","Validation Loss Decreased(0.121540--->0.119289) \t Saving The Model\n","Validation Loss Decreased(0.119289--->0.117109) \t Saving The Model\n","Validation Loss Decreased(0.117109--->0.114996) \t Saving The Model\n","Validation Loss Decreased(0.114996--->0.112945) \t Saving The Model\n","Validation Loss Decreased(0.112945--->0.110963) \t Saving The Model\n","Validation Loss Decreased(0.110963--->0.109043) \t Saving The Model\n","Validation Loss Decreased(0.109043--->0.107190) \t Saving The Model\n","Validation Loss Decreased(0.107190--->0.105404) \t Saving The Model\n","Validation Loss Decreased(0.105404--->0.103680) \t Saving The Model\n","Validation Loss Decreased(0.103680--->0.102013) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.32888388137022656 \t Validation Loss: 0.10040611028671265 \n","\n","Validation Loss Decreased(0.102013--->0.100406) \t Saving The Model\n","Validation Loss Decreased(0.100406--->0.098859) \t Saving The Model\n","Validation Loss Decreased(0.098859--->0.097368) \t Saving The Model\n","Validation Loss Decreased(0.097368--->0.095931) \t Saving The Model\n","Validation Loss Decreased(0.095931--->0.094549) \t Saving The Model\n","Validation Loss Decreased(0.094549--->0.093217) \t Saving The Model\n","Validation Loss Decreased(0.093217--->0.091933) \t Saving The Model\n","Validation Loss Decreased(0.091933--->0.090698) \t Saving The Model\n","Validation Loss Decreased(0.090698--->0.089510) \t Saving The Model\n","Validation Loss Decreased(0.089510--->0.088366) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.3627261072397232 \t Validation Loss: 0.08726335316896439 \n","\n","Validation Loss Decreased(0.088366--->0.087263) \t Saving The Model\n","Validation Loss Decreased(0.087263--->0.086201) \t Saving The Model\n","Validation Loss Decreased(0.086201--->0.085179) \t Saving The Model\n","Validation Loss Decreased(0.085179--->0.084195) \t Saving The Model\n","Validation Loss Decreased(0.084195--->0.083248) \t Saving The Model\n","Validation Loss Decreased(0.083248--->0.082334) \t Saving The Model\n","Validation Loss Decreased(0.082334--->0.081453) \t Saving The Model\n","Validation Loss Decreased(0.081453--->0.080602) \t Saving The Model\n","Validation Loss Decreased(0.080602--->0.079783) \t Saving The Model\n","Validation Loss Decreased(0.079783--->0.078992) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.3876066555579503 \t Validation Loss: 0.07822888344526291 \n","\n","Validation Loss Decreased(0.078992--->0.078229) \t Saving The Model\n","Validation Loss Decreased(0.078229--->0.077491) \t Saving The Model\n","Validation Loss Decreased(0.077491--->0.076778) \t Saving The Model\n","Validation Loss Decreased(0.076778--->0.076089) \t Saving The Model\n","Validation Loss Decreased(0.076089--->0.075423) \t Saving The Model\n","Validation Loss Decreased(0.075423--->0.074779) \t Saving The Model\n","Validation Loss Decreased(0.074779--->0.074155) \t Saving The Model\n","Validation Loss Decreased(0.074155--->0.073550) \t Saving The Model\n","Validation Loss Decreased(0.073550--->0.072964) \t Saving The Model\n","Validation Loss Decreased(0.072964--->0.072396) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.4051954746246338 \t Validation Loss: 0.07184416800737381 \n","\n","Validation Loss Decreased(0.072396--->0.071844) \t Saving The Model\n","Validation Loss Decreased(0.071844--->0.071310) \t Saving The Model\n","Validation Loss Decreased(0.071310--->0.070790) \t Saving The Model\n","Validation Loss Decreased(0.070790--->0.070286) \t Saving The Model\n","Validation Loss Decreased(0.070286--->0.069795) \t Saving The Model\n","Validation Loss Decreased(0.069795--->0.069317) \t Saving The Model\n","Validation Loss Decreased(0.069317--->0.068852) \t Saving The Model\n","Validation Loss Decreased(0.068852--->0.068400) \t Saving The Model\n","Validation Loss Decreased(0.068400--->0.067959) \t Saving The Model\n","Validation Loss Decreased(0.067959--->0.067530) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.4175150493780772 \t Validation Loss: 0.06711117178201675 \n","\n","Validation Loss Decreased(0.067530--->0.067111) \t Saving The Model\n","Validation Loss Decreased(0.067111--->0.066702) \t Saving The Model\n","Validation Loss Decreased(0.066702--->0.066303) \t Saving The Model\n","Validation Loss Decreased(0.066303--->0.065912) \t Saving The Model\n","Validation Loss Decreased(0.065912--->0.065531) \t Saving The Model\n","Validation Loss Decreased(0.065531--->0.065159) \t Saving The Model\n","Validation Loss Decreased(0.065159--->0.064795) \t Saving The Model\n","Validation Loss Decreased(0.064795--->0.064438) \t Saving The Model\n","Validation Loss Decreased(0.064438--->0.064089) \t Saving The Model\n","Validation Loss Decreased(0.064089--->0.063747) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.4262143274148305 \t Validation Loss: 0.06341209262609482 \n","\n","Validation Loss Decreased(0.063747--->0.063412) \t Saving The Model\n","Validation Loss Decreased(0.063412--->0.063084) \t Saving The Model\n","Validation Loss Decreased(0.063084--->0.062762) \t Saving The Model\n","Validation Loss Decreased(0.062762--->0.062446) \t Saving The Model\n","Validation Loss Decreased(0.062446--->0.062136) \t Saving The Model\n","Validation Loss Decreased(0.062136--->0.061831) \t Saving The Model\n","Validation Loss Decreased(0.061831--->0.061532) \t Saving The Model\n","Validation Loss Decreased(0.061532--->0.061238) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:46:29,426]\u001b[0m Trial 11 finished with value: 148.57869497753583 and parameters: {'batch_size': 512, 'learning_rate': 0.31137875257396325, 'hidden_size': 32, 'num_layers': 4}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.061238--->0.060950) \t Saving The Model\n","Validation Loss Decreased(0.060950--->0.060666) \t Saving The Model\n","best loss for the trial =  0.060665734112262726\n"," \n","SMAPE : 148.57869497753583\n","\n"," Epoch 0 \t Training Loss: 0.0638086295997103 \t Validation Loss: 0.3988932967185974 \n","\n","Validation Loss Decreased(inf--->0.398893) \t Saving The Model\n","Validation Loss Decreased(0.398893--->0.272338) \t Saving The Model\n","Validation Loss Decreased(0.272338--->0.261832) \t Saving The Model\n","Validation Loss Decreased(0.261832--->0.238788) \t Saving The Model\n","Validation Loss Decreased(0.238788--->0.184539) \t Saving The Model\n","Validation Loss Decreased(0.184539--->0.144448) \t Saving The Model\n","Validation Loss Decreased(0.144448--->0.134632) \t Saving The Model\n","Validation Loss Decreased(0.134632--->0.114330) \t Saving The Model\n","Validation Loss Decreased(0.114330--->0.094510) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.025638864375650883 \t Validation Loss: 0.10674060881137848 \n","\n","\n"," Epoch 20 \t Training Loss: 0.023192882537841797 \t Validation Loss: 0.11910396069288254 \n","\n","\n"," Epoch 30 \t Training Loss: 0.022874215229724843 \t Validation Loss: 0.12474051862955093 \n","\n","\n"," Epoch 40 \t Training Loss: 0.022662268951535225 \t Validation Loss: 0.13682085275650024 \n","\n","\n"," Epoch 50 \t Training Loss: 0.022533637161056202 \t Validation Loss: 0.14389291405677795 \n","\n","\n"," Epoch 60 \t Training Loss: 0.022435852404062945 \t Validation Loss: 0.1450629085302353 \n","\n","\n"," Epoch 70 \t Training Loss: 0.02227046185483535 \t Validation Loss: 0.13682030141353607 \n","\n","\n"," Epoch 80 \t Training Loss: 0.022323989775031805 \t Validation Loss: 0.14674979448318481 \n","\n","\n"," Epoch 90 \t Training Loss: 0.02226685794691245 \t Validation Loss: 0.14667654037475586 \n","\n","\n"," Epoch 100 \t Training Loss: 0.022306555105994146 \t Validation Loss: 0.1338644176721573 \n","\n","\n"," Epoch 110 \t Training Loss: 0.022249316951880854 \t Validation Loss: 0.1439860463142395 \n","\n","\n"," Epoch 120 \t Training Loss: 0.02221076749265194 \t Validation Loss: 0.14798225462436676 \n","\n","\n"," Epoch 130 \t Training Loss: 0.022171704874684412 \t Validation Loss: 0.14840371906757355 \n","\n","\n"," Epoch 140 \t Training Loss: 0.022140542045235634 \t Validation Loss: 0.1479896754026413 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:46:43,446]\u001b[0m Trial 12 finished with value: 43.68449147626929 and parameters: {'batch_size': 512, 'learning_rate': 0.008156396311457238, 'hidden_size': 128, 'num_layers': 3}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.09450971335172653\n"," \n","SMAPE : 43.68449147626929\n","\n"," Epoch 0 \t Training Loss: 9880.198393335255 \t Validation Loss: 7303.48974609375 \n","\n","Validation Loss Decreased(inf--->7303.489746) \t Saving The Model\n","Validation Loss Decreased(7303.489746--->6860.288574) \t Saving The Model\n","Validation Loss Decreased(6860.288574--->2217.965332) \t Saving The Model\n","Validation Loss Decreased(2217.965332--->534.392944) \t Saving The Model\n","Validation Loss Decreased(534.392944--->518.027344) \t Saving The Model\n","Validation Loss Decreased(518.027344--->187.382324) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 271.7632191975911 \t Validation Loss: 594.869140625 \n","\n","Validation Loss Decreased(187.382324--->83.750420) \t Saving The Model\n","Validation Loss Decreased(83.750420--->34.171219) \t Saving The Model\n","Validation Loss Decreased(34.171219--->15.511419) \t Saving The Model\n","Validation Loss Decreased(15.511419--->8.457152) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 17.700291474660236 \t Validation Loss: 6.667885780334473 \n","\n","Validation Loss Decreased(8.457152--->6.667886) \t Saving The Model\n","Validation Loss Decreased(6.667886--->1.121029) \t Saving The Model\n","Validation Loss Decreased(1.121029--->0.689811) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 1.2220661838849385 \t Validation Loss: 2.0178022384643555 \n","\n","Validation Loss Decreased(0.689811--->0.206615) \t Saving The Model\n","Validation Loss Decreased(0.206615--->0.108839) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.0635223841915528 \t Validation Loss: 0.13158631324768066 \n","\n","\n"," Epoch 50 \t Training Loss: 0.08081124722957611 \t Validation Loss: 0.2832753360271454 \n","\n","\n"," Epoch 60 \t Training Loss: 0.07645208885272343 \t Validation Loss: 0.24033737182617188 \n","\n","\n"," Epoch 70 \t Training Loss: 0.08350300292174022 \t Validation Loss: 0.22189517319202423 \n","\n","\n"," Epoch 80 \t Training Loss: 0.09804591536521912 \t Validation Loss: 0.20740315318107605 \n","\n","\n"," Epoch 90 \t Training Loss: 0.1151997980972131 \t Validation Loss: 0.1897735893726349 \n","\n","\n"," Epoch 100 \t Training Loss: 0.1374968389670054 \t Validation Loss: 0.16953188180923462 \n","\n","\n"," Epoch 110 \t Training Loss: 0.16722484678030014 \t Validation Loss: 0.14623744785785675 \n","\n","\n"," Epoch 120 \t Training Loss: 0.20800934731960297 \t Validation Loss: 0.1194351390004158 \n","\n","Validation Loss Decreased(0.108839--->0.107692) \t Saving The Model\n","Validation Loss Decreased(0.107692--->0.104669) \t Saving The Model\n","Validation Loss Decreased(0.104669--->0.101618) \t Saving The Model\n","Validation Loss Decreased(0.101618--->0.098533) \t Saving The Model\n","Validation Loss Decreased(0.098533--->0.095414) \t Saving The Model\n","Validation Loss Decreased(0.095414--->0.092268) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.26613150785366696 \t Validation Loss: 0.0890999510884285 \n","\n","Validation Loss Decreased(0.092268--->0.089100) \t Saving The Model\n","Validation Loss Decreased(0.089100--->0.085908) \t Saving The Model\n","Validation Loss Decreased(0.085908--->0.082698) \t Saving The Model\n","Validation Loss Decreased(0.082698--->0.079463) \t Saving The Model\n","Validation Loss Decreased(0.079463--->0.076218) \t Saving The Model\n","Validation Loss Decreased(0.076218--->0.072968) \t Saving The Model\n","Validation Loss Decreased(0.072968--->0.069718) \t Saving The Model\n","Validation Loss Decreased(0.069718--->0.066470) \t Saving The Model\n","Validation Loss Decreased(0.066470--->0.063235) \t Saving The Model\n","Validation Loss Decreased(0.063235--->0.060024) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.35283642013867694 \t Validation Loss: 0.0568409338593483 \n","\n","Validation Loss Decreased(0.060024--->0.056841) \t Saving The Model\n","Validation Loss Decreased(0.056841--->0.053695) \t Saving The Model\n","Validation Loss Decreased(0.053695--->0.050604) \t Saving The Model\n","Validation Loss Decreased(0.050604--->0.047582) \t Saving The Model\n","Validation Loss Decreased(0.047582--->0.044646) \t Saving The Model\n","Validation Loss Decreased(0.044646--->0.041815) \t Saving The Model\n","Validation Loss Decreased(0.041815--->0.039107) \t Saving The Model\n","Validation Loss Decreased(0.039107--->0.036546) \t Saving The Model\n","Validation Loss Decreased(0.036546--->0.034163) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:47:31,562]\u001b[0m Trial 13 finished with value: 48.546180889244035 and parameters: {'batch_size': 512, 'learning_rate': 0.5581206393630049, 'hidden_size': 256, 'num_layers': 5}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.034163--->0.031991) \t Saving The Model\n","best loss for the trial =  0.031991273164749146\n"," \n","SMAPE : 48.546180889244035\n","\n"," Epoch 0 \t Training Loss: 0.172253906223159 \t Validation Loss: 0.40067887902259824 \n","\n","Validation Loss Decreased(inf--->0.400679) \t Saving The Model\n","Validation Loss Decreased(0.400679--->0.339234) \t Saving The Model\n","Validation Loss Decreased(0.339234--->0.338194) \t Saving The Model\n","Validation Loss Decreased(0.338194--->0.328732) \t Saving The Model\n","Validation Loss Decreased(0.328732--->0.319278) \t Saving The Model\n","Validation Loss Decreased(0.319278--->0.308533) \t Saving The Model\n","Validation Loss Decreased(0.308533--->0.297457) \t Saving The Model\n","Validation Loss Decreased(0.297457--->0.286022) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.020206750236684456 \t Validation Loss: 0.274296435713768 \n","\n","Validation Loss Decreased(0.286022--->0.274296) \t Saving The Model\n","Validation Loss Decreased(0.274296--->0.262400) \t Saving The Model\n","Validation Loss Decreased(0.262400--->0.250396) \t Saving The Model\n","Validation Loss Decreased(0.250396--->0.238353) \t Saving The Model\n","Validation Loss Decreased(0.238353--->0.226328) \t Saving The Model\n","Validation Loss Decreased(0.226328--->0.214374) \t Saving The Model\n","Validation Loss Decreased(0.214374--->0.202538) \t Saving The Model\n","Validation Loss Decreased(0.202538--->0.190864) \t Saving The Model\n","Validation Loss Decreased(0.190864--->0.179391) \t Saving The Model\n","Validation Loss Decreased(0.179391--->0.168157) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.012862310473428807 \t Validation Loss: 0.1571968302130699 \n","\n","Validation Loss Decreased(0.168157--->0.157197) \t Saving The Model\n","Validation Loss Decreased(0.157197--->0.146543) \t Saving The Model\n","Validation Loss Decreased(0.146543--->0.136225) \t Saving The Model\n","Validation Loss Decreased(0.136225--->0.126273) \t Saving The Model\n","Validation Loss Decreased(0.126273--->0.116712) \t Saving The Model\n","Validation Loss Decreased(0.116712--->0.107566) \t Saving The Model\n","Validation Loss Decreased(0.107566--->0.098856) \t Saving The Model\n","Validation Loss Decreased(0.098856--->0.090601) \t Saving The Model\n","Validation Loss Decreased(0.090601--->0.082815) \t Saving The Model\n","Validation Loss Decreased(0.082815--->0.075508) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.006928558896470349 \t Validation Loss: 0.06868912428617477 \n","\n","Validation Loss Decreased(0.075508--->0.068689) \t Saving The Model\n","Validation Loss Decreased(0.068689--->0.062360) \t Saving The Model\n","Validation Loss Decreased(0.062360--->0.056520) \t Saving The Model\n","Validation Loss Decreased(0.056520--->0.051163) \t Saving The Model\n","Validation Loss Decreased(0.051163--->0.046278) \t Saving The Model\n","Validation Loss Decreased(0.046278--->0.041853) \t Saving The Model\n","Validation Loss Decreased(0.041853--->0.037869) \t Saving The Model\n","Validation Loss Decreased(0.037869--->0.034304) \t Saving The Model\n","Validation Loss Decreased(0.034304--->0.031135) \t Saving The Model\n","Validation Loss Decreased(0.031135--->0.028336) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.003998004582535941 \t Validation Loss: 0.025878078117966653 \n","\n","Validation Loss Decreased(0.028336--->0.025878) \t Saving The Model\n","Validation Loss Decreased(0.025878--->0.023733) \t Saving The Model\n","Validation Loss Decreased(0.023733--->0.021872) \t Saving The Model\n","Validation Loss Decreased(0.021872--->0.020266) \t Saving The Model\n","Validation Loss Decreased(0.020266--->0.018888) \t Saving The Model\n","Validation Loss Decreased(0.018888--->0.017710) \t Saving The Model\n","Validation Loss Decreased(0.017710--->0.016709) \t Saving The Model\n","Validation Loss Decreased(0.016709--->0.015860) \t Saving The Model\n","Validation Loss Decreased(0.015860--->0.015144) \t Saving The Model\n","Validation Loss Decreased(0.015144--->0.014541) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.003241055987018626 \t Validation Loss: 0.014034298434853553 \n","\n","Validation Loss Decreased(0.014541--->0.014034) \t Saving The Model\n","Validation Loss Decreased(0.014034--->0.013610) \t Saving The Model\n","Validation Loss Decreased(0.013610--->0.013254) \t Saving The Model\n","Validation Loss Decreased(0.013254--->0.012957) \t Saving The Model\n","Validation Loss Decreased(0.012957--->0.012709) \t Saving The Model\n","Validation Loss Decreased(0.012709--->0.012501) \t Saving The Model\n","Validation Loss Decreased(0.012501--->0.012328) \t Saving The Model\n","Validation Loss Decreased(0.012328--->0.012182) \t Saving The Model\n","Validation Loss Decreased(0.012182--->0.012061) \t Saving The Model\n","Validation Loss Decreased(0.012061--->0.011959) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0031392918313940753 \t Validation Loss: 0.01187324821949005 \n","\n","Validation Loss Decreased(0.011959--->0.011873) \t Saving The Model\n","Validation Loss Decreased(0.011873--->0.011801) \t Saving The Model\n","Validation Loss Decreased(0.011801--->0.011741) \t Saving The Model\n","Validation Loss Decreased(0.011741--->0.011689) \t Saving The Model\n","Validation Loss Decreased(0.011689--->0.011646) \t Saving The Model\n","Validation Loss Decreased(0.011646--->0.011609) \t Saving The Model\n","Validation Loss Decreased(0.011609--->0.011577) \t Saving The Model\n","Validation Loss Decreased(0.011577--->0.011550) \t Saving The Model\n","Validation Loss Decreased(0.011550--->0.011527) \t Saving The Model\n","Validation Loss Decreased(0.011527--->0.011506) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0031358212578197707 \t Validation Loss: 0.011487995181232691 \n","\n","Validation Loss Decreased(0.011506--->0.011488) \t Saving The Model\n","Validation Loss Decreased(0.011488--->0.011472) \t Saving The Model\n","Validation Loss Decreased(0.011472--->0.011458) \t Saving The Model\n","Validation Loss Decreased(0.011458--->0.011444) \t Saving The Model\n","Validation Loss Decreased(0.011444--->0.011432) \t Saving The Model\n","Validation Loss Decreased(0.011432--->0.011421) \t Saving The Model\n","Validation Loss Decreased(0.011421--->0.011411) \t Saving The Model\n","Validation Loss Decreased(0.011411--->0.011400) \t Saving The Model\n","Validation Loss Decreased(0.011400--->0.011391) \t Saving The Model\n","Validation Loss Decreased(0.011391--->0.011381) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.003147245645050134 \t Validation Loss: 0.011371880397200584 \n","\n","Validation Loss Decreased(0.011381--->0.011372) \t Saving The Model\n","Validation Loss Decreased(0.011372--->0.011363) \t Saving The Model\n","Validation Loss Decreased(0.011363--->0.011354) \t Saving The Model\n","Validation Loss Decreased(0.011354--->0.011345) \t Saving The Model\n","Validation Loss Decreased(0.011345--->0.011335) \t Saving The Model\n","Validation Loss Decreased(0.011335--->0.011326) \t Saving The Model\n","Validation Loss Decreased(0.011326--->0.011317) \t Saving The Model\n","Validation Loss Decreased(0.011317--->0.011308) \t Saving The Model\n","Validation Loss Decreased(0.011308--->0.011299) \t Saving The Model\n","Validation Loss Decreased(0.011299--->0.011289) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.0031643468619222404 \t Validation Loss: 0.011279654875397681 \n","\n","Validation Loss Decreased(0.011289--->0.011280) \t Saving The Model\n","Validation Loss Decreased(0.011280--->0.011270) \t Saving The Model\n","Validation Loss Decreased(0.011270--->0.011260) \t Saving The Model\n","Validation Loss Decreased(0.011260--->0.011250) \t Saving The Model\n","Validation Loss Decreased(0.011250--->0.011240) \t Saving The Model\n","Validation Loss Decreased(0.011240--->0.011230) \t Saving The Model\n","Validation Loss Decreased(0.011230--->0.011220) \t Saving The Model\n","Validation Loss Decreased(0.011220--->0.011209) \t Saving The Model\n","Validation Loss Decreased(0.011209--->0.011199) \t Saving The Model\n","Validation Loss Decreased(0.011199--->0.011188) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.0031883892640507836 \t Validation Loss: 0.011176944710314274 \n","\n","Validation Loss Decreased(0.011188--->0.011177) \t Saving The Model\n","Validation Loss Decreased(0.011177--->0.011166) \t Saving The Model\n","Validation Loss Decreased(0.011166--->0.011155) \t Saving The Model\n","Validation Loss Decreased(0.011155--->0.011143) \t Saving The Model\n","Validation Loss Decreased(0.011143--->0.011132) \t Saving The Model\n","Validation Loss Decreased(0.011132--->0.011120) \t Saving The Model\n","Validation Loss Decreased(0.011120--->0.011108) \t Saving The Model\n","Validation Loss Decreased(0.011108--->0.011096) \t Saving The Model\n","Validation Loss Decreased(0.011096--->0.011084) \t Saving The Model\n","Validation Loss Decreased(0.011084--->0.011071) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.0032257648023460207 \t Validation Loss: 0.011058818269520997 \n","\n","Validation Loss Decreased(0.011071--->0.011059) \t Saving The Model\n","Validation Loss Decreased(0.011059--->0.011046) \t Saving The Model\n","Validation Loss Decreased(0.011046--->0.011033) \t Saving The Model\n","Validation Loss Decreased(0.011033--->0.011020) \t Saving The Model\n","Validation Loss Decreased(0.011020--->0.011007) \t Saving The Model\n","Validation Loss Decreased(0.011007--->0.010994) \t Saving The Model\n","Validation Loss Decreased(0.010994--->0.010981) \t Saving The Model\n","Validation Loss Decreased(0.010981--->0.010967) \t Saving The Model\n","Validation Loss Decreased(0.010967--->0.010954) \t Saving The Model\n","Validation Loss Decreased(0.010954--->0.010941) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.003292167635572696 \t Validation Loss: 0.010927129629999399 \n","\n","Validation Loss Decreased(0.010941--->0.010927) \t Saving The Model\n","Validation Loss Decreased(0.010927--->0.010914) \t Saving The Model\n","Validation Loss Decreased(0.010914--->0.010900) \t Saving The Model\n","Validation Loss Decreased(0.010900--->0.010887) \t Saving The Model\n","Validation Loss Decreased(0.010887--->0.010874) \t Saving The Model\n","Validation Loss Decreased(0.010874--->0.010861) \t Saving The Model\n","Validation Loss Decreased(0.010861--->0.010848) \t Saving The Model\n","Validation Loss Decreased(0.010848--->0.010835) \t Saving The Model\n","Validation Loss Decreased(0.010835--->0.010823) \t Saving The Model\n","Validation Loss Decreased(0.010823--->0.010812) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.0034226687250338726 \t Validation Loss: 0.01080030994489789 \n","\n","Validation Loss Decreased(0.010812--->0.010800) \t Saving The Model\n","Validation Loss Decreased(0.010800--->0.010790) \t Saving The Model\n","Validation Loss Decreased(0.010790--->0.010780) \t Saving The Model\n","Validation Loss Decreased(0.010780--->0.010770) \t Saving The Model\n","Validation Loss Decreased(0.010770--->0.010762) \t Saving The Model\n","Validation Loss Decreased(0.010762--->0.010754) \t Saving The Model\n","Validation Loss Decreased(0.010754--->0.010747) \t Saving The Model\n","Validation Loss Decreased(0.010747--->0.010741) \t Saving The Model\n","Validation Loss Decreased(0.010741--->0.010737) \t Saving The Model\n","Validation Loss Decreased(0.010737--->0.010733) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.003672638376156101 \t Validation Loss: 0.010730767901986838 \n","\n","Validation Loss Decreased(0.010733--->0.010731) \t Saving The Model\n","Validation Loss Decreased(0.010731--->0.010730) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:49:09,500]\u001b[0m Trial 14 finished with value: 28.725816820844933 and parameters: {'batch_size': 64, 'learning_rate': 0.007569517785721148, 'hidden_size': 512, 'num_layers': 1}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.010729702189564705\n"," \n","SMAPE : 28.725816820844933\n","\n"," Epoch 0 \t Training Loss: 0.028811361561747618 \t Validation Loss: 0.40829541683197024 \n","\n","Validation Loss Decreased(inf--->0.408295) \t Saving The Model\n","Validation Loss Decreased(0.408295--->0.320318) \t Saving The Model\n","Validation Loss Decreased(0.320318--->0.282571) \t Saving The Model\n","Validation Loss Decreased(0.282571--->0.165726) \t Saving The Model\n","Validation Loss Decreased(0.165726--->0.085573) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.01910897014349757 \t Validation Loss: 0.3183372259140015 \n","\n","\n"," Epoch 20 \t Training Loss: 0.013882942803320475 \t Validation Loss: 0.15815426111221315 \n","\n","Validation Loss Decreased(0.085573--->0.021615) \t Saving The Model\n","Validation Loss Decreased(0.021615--->0.020332) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.010804793993702332 \t Validation Loss: 0.12434810250997544 \n","\n","Validation Loss Decreased(0.020332--->0.019964) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.003965195624550688 \t Validation Loss: 0.03858522418886423 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0036146914098935667 \t Validation Loss: 0.03582841269671917 \n","\n","Validation Loss Decreased(0.019964--->0.019940) \t Saving The Model\n","Validation Loss Decreased(0.019940--->0.019734) \t Saving The Model\n","Validation Loss Decreased(0.019734--->0.019527) \t Saving The Model\n","Validation Loss Decreased(0.019527--->0.019321) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0034251558405230752 \t Validation Loss: 0.033761830627918245 \n","\n","Validation Loss Decreased(0.019321--->0.019117) \t Saving The Model\n","Validation Loss Decreased(0.019117--->0.018916) \t Saving The Model\n","Validation Loss Decreased(0.018916--->0.018719) \t Saving The Model\n","Validation Loss Decreased(0.018719--->0.018528) \t Saving The Model\n","Validation Loss Decreased(0.018528--->0.018343) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.003303675196366385 \t Validation Loss: 0.032024487294256686 \n","\n","Validation Loss Decreased(0.018343--->0.018165) \t Saving The Model\n","Validation Loss Decreased(0.018165--->0.017993) \t Saving The Model\n","Validation Loss Decreased(0.017993--->0.017829) \t Saving The Model\n","Validation Loss Decreased(0.017829--->0.017672) \t Saving The Model\n","Validation Loss Decreased(0.017672--->0.017523) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0032179061534407085 \t Validation Loss: 0.03062262311577797 \n","\n","Validation Loss Decreased(0.017523--->0.017381) \t Saving The Model\n","Validation Loss Decreased(0.017381--->0.017246) \t Saving The Model\n","Validation Loss Decreased(0.017246--->0.017119) \t Saving The Model\n","Validation Loss Decreased(0.017119--->0.016998) \t Saving The Model\n","Validation Loss Decreased(0.016998--->0.016884) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.003154230626751087 \t Validation Loss: 0.02952237520366907 \n","\n","Validation Loss Decreased(0.016884--->0.016777) \t Saving The Model\n","Validation Loss Decreased(0.016777--->0.016676) \t Saving The Model\n","Validation Loss Decreased(0.016676--->0.016581) \t Saving The Model\n","Validation Loss Decreased(0.016581--->0.016493) \t Saving The Model\n","Validation Loss Decreased(0.016493--->0.016410) \t Saving The Model\n","\n"," Epoch 100 \t Training Loss: 0.003108301051906892 \t Validation Loss: 0.02869664113968611 \n","\n","Validation Loss Decreased(0.016410--->0.016332) \t Saving The Model\n","Validation Loss Decreased(0.016332--->0.016259) \t Saving The Model\n","Validation Loss Decreased(0.016259--->0.016192) \t Saving The Model\n","Validation Loss Decreased(0.016192--->0.016129) \t Saving The Model\n","Validation Loss Decreased(0.016129--->0.016071) \t Saving The Model\n","\n"," Epoch 110 \t Training Loss: 0.003076852161575516 \t Validation Loss: 0.028101998567581176 \n","\n","Validation Loss Decreased(0.016071--->0.016017) \t Saving The Model\n","Validation Loss Decreased(0.016017--->0.015967) \t Saving The Model\n","Validation Loss Decreased(0.015967--->0.015921) \t Saving The Model\n","Validation Loss Decreased(0.015921--->0.015879) \t Saving The Model\n","Validation Loss Decreased(0.015879--->0.015839) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.0030556560276636447 \t Validation Loss: 0.02767672948539257 \n","\n","Validation Loss Decreased(0.015839--->0.015804) \t Saving The Model\n","Validation Loss Decreased(0.015804--->0.015771) \t Saving The Model\n","Validation Loss Decreased(0.015771--->0.015741) \t Saving The Model\n","Validation Loss Decreased(0.015741--->0.015714) \t Saving The Model\n","Validation Loss Decreased(0.015714--->0.015690) \t Saving The Model\n","\n"," Epoch 130 \t Training Loss: 0.003040661138948053 \t Validation Loss: 0.027361521497368813 \n","\n","Validation Loss Decreased(0.015690--->0.015668) \t Saving The Model\n","Validation Loss Decreased(0.015668--->0.015648) \t Saving The Model\n","Validation Loss Decreased(0.015648--->0.015631) \t Saving The Model\n","Validation Loss Decreased(0.015631--->0.015616) \t Saving The Model\n","Validation Loss Decreased(0.015616--->0.015602) \t Saving The Model\n","\n"," Epoch 140 \t Training Loss: 0.003028892138900119 \t Validation Loss: 0.02711343578994274 \n","\n","Validation Loss Decreased(0.015602--->0.015591) \t Saving The Model\n","Validation Loss Decreased(0.015591--->0.015582) \t Saving The Model\n","Validation Loss Decreased(0.015582--->0.015574) \t Saving The Model\n","Validation Loss Decreased(0.015574--->0.015569) \t Saving The Model\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:50:46,653]\u001b[0m Trial 15 finished with value: 48.48673216775724 and parameters: {'batch_size': 64, 'learning_rate': 0.0004255225065631496, 'hidden_size': 512, 'num_layers': 1}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Validation Loss Decreased(0.015569--->0.015564) \t Saving The Model\n","best loss for the trial =  0.015564397815614939\n"," \n","SMAPE : 48.48673216775724\n","\n"," Epoch 0 \t Training Loss: 3.966016804985702 \t Validation Loss: 1.3229875564575195 \n","\n","Validation Loss Decreased(inf--->1.322988) \t Saving The Model\n","Validation Loss Decreased(1.322988--->0.192659) \t Saving The Model\n","Validation Loss Decreased(0.192659--->0.113850) \t Saving The Model\n","Validation Loss Decreased(0.113850--->0.113392) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.13989949356764556 \t Validation Loss: 0.10985973626375198 \n","\n","Validation Loss Decreased(0.113392--->0.109860) \t Saving The Model\n","Validation Loss Decreased(0.109860--->0.082333) \t Saving The Model\n","Validation Loss Decreased(0.082333--->0.054084) \t Saving The Model\n","Validation Loss Decreased(0.054084--->0.035932) \t Saving The Model\n","Validation Loss Decreased(0.035932--->0.027839) \t Saving The Model\n","Validation Loss Decreased(0.027839--->0.024675) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.13877101391553878 \t Validation Loss: 0.03041057363152504 \n","\n","\n"," Epoch 30 \t Training Loss: 0.14509777300991117 \t Validation Loss: 0.06873178035020829 \n","\n","\n"," Epoch 40 \t Training Loss: 0.14502213951200246 \t Validation Loss: 0.0718873307108879 \n","\n","\n"," Epoch 50 \t Training Loss: 0.13325243387371302 \t Validation Loss: 0.044902899861335756 \n","\n","\n"," Epoch 60 \t Training Loss: 0.13436329029500485 \t Validation Loss: 0.033682788535952565 \n","\n","\n"," Epoch 70 \t Training Loss: 0.14123301114887 \t Validation Loss: 0.02901162039488554 \n","\n","\n"," Epoch 80 \t Training Loss: 0.14567248951643705 \t Validation Loss: 0.03199760504066944 \n","\n","\n"," Epoch 90 \t Training Loss: 0.14665054567158223 \t Validation Loss: 0.02874360289424658 \n","\n","\n"," Epoch 100 \t Training Loss: 0.14425527770072222 \t Validation Loss: 0.0356050681322813 \n","\n","\n"," Epoch 110 \t Training Loss: 0.14252758417278527 \t Validation Loss: 0.04464791901409626 \n","\n","\n"," Epoch 120 \t Training Loss: 0.1393191611394286 \t Validation Loss: 0.05179651752114296 \n","\n","\n"," Epoch 130 \t Training Loss: 0.13558612260967493 \t Validation Loss: 0.052802805230021475 \n","\n","\n"," Epoch 140 \t Training Loss: 0.1269268576055765 \t Validation Loss: 0.06278897449374199 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:54:02,140]\u001b[0m Trial 16 finished with value: 120.85365441314005 and parameters: {'batch_size': 64, 'learning_rate': 0.0061910186321659, 'hidden_size': 512, 'num_layers': 4}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.02467476427555084\n"," \n","SMAPE : 120.85365441314005\n","\n"," Epoch 0 \t Training Loss: 0.02711202386126388 \t Validation Loss: 0.41632357239723206 \n","\n","Validation Loss Decreased(inf--->0.416324) \t Saving The Model\n","Validation Loss Decreased(0.416324--->0.365875) \t Saving The Model\n","Validation Loss Decreased(0.365875--->0.353059) \t Saving The Model\n","Validation Loss Decreased(0.353059--->0.343065) \t Saving The Model\n","Validation Loss Decreased(0.343065--->0.326749) \t Saving The Model\n","Validation Loss Decreased(0.326749--->0.297785) \t Saving The Model\n","Validation Loss Decreased(0.297785--->0.219864) \t Saving The Model\n","Validation Loss Decreased(0.219864--->0.037399) \t Saving The Model\n","Validation Loss Decreased(0.037399--->0.019965) \t Saving The Model\n","Validation Loss Decreased(0.019965--->0.012513) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.020348701323382557 \t Validation Loss: 0.3219710439443588 \n","\n","\n"," Epoch 20 \t Training Loss: 0.0031864771553955507 \t Validation Loss: 0.021999410539865493 \n","\n","Validation Loss Decreased(0.012513--->0.011126) \t Saving The Model\n","Validation Loss Decreased(0.011126--->0.010798) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.002997079950000625 \t Validation Loss: 0.021970378048717976 \n","\n","\n"," Epoch 40 \t Training Loss: 0.0030149667124078406 \t Validation Loss: 0.02324224654585123 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0030216720865610114 \t Validation Loss: 0.024854848347604274 \n","\n","\n"," Epoch 60 \t Training Loss: 0.0030485704480270213 \t Validation Loss: 0.02654833663254976 \n","\n","\n"," Epoch 70 \t Training Loss: 0.0030588755868848237 \t Validation Loss: 0.027753455005586146 \n","\n","\n"," Epoch 80 \t Training Loss: 0.0030610294295911446 \t Validation Loss: 0.0286706468090415 \n","\n","\n"," Epoch 90 \t Training Loss: 0.00305451823378462 \t Validation Loss: 0.029319268092513083 \n","\n","\n"," Epoch 100 \t Training Loss: 0.003042234531221766 \t Validation Loss: 0.029784817062318326 \n","\n","\n"," Epoch 110 \t Training Loss: 0.0030265590755334413 \t Validation Loss: 0.03014073707163334 \n","\n","\n"," Epoch 120 \t Training Loss: 0.0030090920243310395 \t Validation Loss: 0.030445636808872224 \n","\n","\n"," Epoch 130 \t Training Loss: 0.0029906749373367347 \t Validation Loss: 0.03074616100639105 \n","\n","\n"," Epoch 140 \t Training Loss: 0.0029715863476667435 \t Validation Loss: 0.031089941412210463 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:54:36,002]\u001b[0m Trial 17 finished with value: 54.678469093233986 and parameters: {'batch_size': 64, 'learning_rate': 0.0005646602384097838, 'hidden_size': 64, 'num_layers': 3}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.01079762987792492\n"," \n","SMAPE : 54.678469093233986\n","\n"," Epoch 0 \t Training Loss: 20.16986069479317 \t Validation Loss: 0.5241034865379334 \n","\n","Validation Loss Decreased(inf--->0.524103) \t Saving The Model\n","Validation Loss Decreased(0.524103--->0.093218) \t Saving The Model\n","Validation Loss Decreased(0.093218--->0.074211) \t Saving The Model\n","Validation Loss Decreased(0.074211--->0.028305) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.08185290971305222 \t Validation Loss: 0.023741566482931374 \n","\n","Validation Loss Decreased(0.028305--->0.023742) \t Saving The Model\n","Validation Loss Decreased(0.023742--->0.023724) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.1452196916565299 \t Validation Loss: 0.09423249829560518 \n","\n","\n"," Epoch 30 \t Training Loss: 0.1969699947920162 \t Validation Loss: 0.15297216847538947 \n","\n","\n"," Epoch 40 \t Training Loss: 0.1976115843397565 \t Validation Loss: 0.15580543242394923 \n","\n","\n"," Epoch 50 \t Training Loss: 0.19751981515437364 \t Validation Loss: 0.15776453278958796 \n","\n","\n"," Epoch 60 \t Training Loss: 0.19724302775866817 \t Validation Loss: 0.15955580286681653 \n","\n","\n"," Epoch 70 \t Training Loss: 0.19687929580250058 \t Validation Loss: 0.16132852882146836 \n","\n","\n"," Epoch 80 \t Training Loss: 0.19646600242413115 \t Validation Loss: 0.1631402887403965 \n","\n","\n"," Epoch 90 \t Training Loss: 0.19602117259564694 \t Validation Loss: 0.16501627899706364 \n","\n","\n"," Epoch 100 \t Training Loss: 0.19555464995501098 \t Validation Loss: 0.1669684484601021 \n","\n","\n"," Epoch 110 \t Training Loss: 0.19507158744604566 \t Validation Loss: 0.16899923831224442 \n","\n","\n"," Epoch 120 \t Training Loss: 0.19457670700667223 \t Validation Loss: 0.171108116209507 \n","\n","\n"," Epoch 130 \t Training Loss: 0.19407276107085636 \t Validation Loss: 0.17329051047563554 \n","\n","\n"," Epoch 140 \t Training Loss: 0.19356232662539696 \t Validation Loss: 0.1755399711430073 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:57:49,461]\u001b[0m Trial 18 finished with value: 87.4664102308929 and parameters: {'batch_size': 64, 'learning_rate': 0.013529473581231771, 'hidden_size': 1024, 'num_layers': 1}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.023723940178751946\n"," \n","SMAPE : 87.4664102308929\n","\n"," Epoch 0 \t Training Loss: 0.020734209048532647 \t Validation Loss: 0.1968746915459633 \n","\n","Validation Loss Decreased(inf--->0.196875) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.019721789786126466 \t Validation Loss: 0.2715359032154083 \n","\n","Validation Loss Decreased(0.196875--->0.177804) \t Saving The Model\n","Validation Loss Decreased(0.177804--->0.134419) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.02086343665723689 \t Validation Loss: 0.2586205661296844 \n","\n","Validation Loss Decreased(0.134419--->0.123769) \t Saving The Model\n","Validation Loss Decreased(0.123769--->0.109824) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.011732174150529318 \t Validation Loss: 0.0951853834092617 \n","\n","Validation Loss Decreased(0.109824--->0.095185) \t Saving The Model\n","Validation Loss Decreased(0.095185--->0.070525) \t Saving The Model\n","Validation Loss Decreased(0.070525--->0.042072) \t Saving The Model\n","Validation Loss Decreased(0.042072--->0.036169) \t Saving The Model\n","Validation Loss Decreased(0.036169--->0.027272) \t Saving The Model\n","Validation Loss Decreased(0.027272--->0.019103) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.010164843813981861 \t Validation Loss: 0.0867280714213848 \n","\n","\n"," Epoch 50 \t Training Loss: 0.0030843104475934526 \t Validation Loss: 0.03320432342588901 \n","\n","\n"," Epoch 60 \t Training Loss: 0.004315673232304107 \t Validation Loss: 0.028508829046040775 \n","\n","\n"," Epoch 70 \t Training Loss: 0.004135471854169737 \t Validation Loss: 0.02645983612164855 \n","\n","\n"," Epoch 80 \t Training Loss: 0.004010691162693547 \t Validation Loss: 0.027584765572100878 \n","\n","\n"," Epoch 90 \t Training Loss: 0.0038864409256348154 \t Validation Loss: 0.03268057620152831 \n","\n","\n"," Epoch 100 \t Training Loss: 0.0038115630759421038 \t Validation Loss: 0.0355710944160819 \n","\n","\n"," Epoch 110 \t Training Loss: 0.003743030465375341 \t Validation Loss: 0.02967711854726076 \n","\n","\n"," Epoch 120 \t Training Loss: 0.0035855201405865953 \t Validation Loss: 0.029156835097819567 \n","\n","\n"," Epoch 130 \t Training Loss: 0.0037267689855070784 \t Validation Loss: 0.03984646871685982 \n","\n","\n"," Epoch 140 \t Training Loss: 0.003967664512583724 \t Validation Loss: 0.02875113394111395 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 02:58:55,224]\u001b[0m Trial 19 finished with value: 52.29473288255274 and parameters: {'batch_size': 64, 'learning_rate': 0.004821323191917941, 'hidden_size': 128, 'num_layers': 5}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.019103227369487285\n"," \n","SMAPE : 52.29473288255274\n","\n"," Epoch 0 \t Training Loss: 360.69688878506423 \t Validation Loss: 331.2477315266927 \n","\n","Validation Loss Decreased(inf--->331.247732) \t Saving The Model\n","Validation Loss Decreased(331.247732--->53.426118) \t Saving The Model\n","Validation Loss Decreased(53.426118--->9.879344) \t Saving The Model\n","Validation Loss Decreased(9.879344--->5.751880) \t Saving The Model\n","Validation Loss Decreased(5.751880--->0.162176) \t Saving The Model\n","Validation Loss Decreased(0.162176--->0.080627) \t Saving The Model\n","Validation Loss Decreased(0.080627--->0.043905) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.1320703058503568 \t Validation Loss: 0.193563645084699 \n","\n","Validation Loss Decreased(0.043905--->0.027830) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.6508351661264896 \t Validation Loss: 0.7218275467554728 \n","\n","Validation Loss Decreased(0.027830--->0.023279) \t Saving The Model\n","Validation Loss Decreased(0.023279--->0.022368) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.08196089463308454 \t Validation Loss: 0.025483746236811083 \n","\n","\n"," Epoch 40 \t Training Loss: 0.07775540687434841 \t Validation Loss: 0.025385739902655285 \n","\n","\n"," Epoch 50 \t Training Loss: 0.22356851473450662 \t Validation Loss: 0.1028438334663709 \n","\n","\n"," Epoch 60 \t Training Loss: 1.1454310275614261 \t Validation Loss: 1.1174931923548381 \n","\n","\n"," Epoch 70 \t Training Loss: 0.18379537351429462 \t Validation Loss: 0.033202189641694226 \n","\n","\n"," Epoch 80 \t Training Loss: 0.26099552065134046 \t Validation Loss: 0.09099740286668141 \n","\n","\n"," Epoch 90 \t Training Loss: 1.1523478351533414 \t Validation Loss: 1.6679935455322266 \n","\n","\n"," Epoch 100 \t Training Loss: 4.015582650899887 \t Validation Loss: 5.718595186869304 \n","\n","\n"," Epoch 110 \t Training Loss: 3.823367716744542 \t Validation Loss: 5.520127773284912 \n","\n","\n"," Epoch 120 \t Training Loss: 3.834509438276291 \t Validation Loss: 5.533865610758464 \n","\n","\n"," Epoch 130 \t Training Loss: 3.8468805637210606 \t Validation Loss: 5.534866650899251 \n","\n","\n"," Epoch 140 \t Training Loss: 3.7121190309524534 \t Validation Loss: 4.425567468007405 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 03:00:45,370]\u001b[0m Trial 20 finished with value: 156.7493730170021 and parameters: {'batch_size': 128, 'learning_rate': 0.09299839416130128, 'hidden_size': 512, 'num_layers': 3}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.022367841564118862\n"," \n","SMAPE : 156.7493730170021\n","\n"," Epoch 0 \t Training Loss: 0.05203091073781252 \t Validation Loss: 0.28330159187316895 \n","\n","Validation Loss Decreased(inf--->0.283302) \t Saving The Model\n","Validation Loss Decreased(0.283302--->0.203721) \t Saving The Model\n","Validation Loss Decreased(0.203721--->0.190041) \t Saving The Model\n","Validation Loss Decreased(0.190041--->0.170079) \t Saving The Model\n","Validation Loss Decreased(0.170079--->0.160038) \t Saving The Model\n","Validation Loss Decreased(0.160038--->0.147324) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.024702749447897077 \t Validation Loss: 0.12130771577358246 \n","\n","Validation Loss Decreased(0.147324--->0.121308) \t Saving The Model\n","Validation Loss Decreased(0.121308--->0.090376) \t Saving The Model\n","Validation Loss Decreased(0.090376--->0.078663) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.022948756348341703 \t Validation Loss: 0.11146867275238037 \n","\n","\n"," Epoch 30 \t Training Loss: 0.022600495256483555 \t Validation Loss: 0.12681573629379272 \n","\n","\n"," Epoch 40 \t Training Loss: 0.022492908174172044 \t Validation Loss: 0.13037532567977905 \n","\n","\n"," Epoch 50 \t Training Loss: 0.02240920198770861 \t Validation Loss: 0.1325206607580185 \n","\n","\n"," Epoch 60 \t Training Loss: 0.022353738623981673 \t Validation Loss: 0.13479679822921753 \n","\n","\n"," Epoch 70 \t Training Loss: 0.022308717171351116 \t Validation Loss: 0.13654333353042603 \n","\n","\n"," Epoch 80 \t Training Loss: 0.02227249532006681 \t Validation Loss: 0.13809852302074432 \n","\n","\n"," Epoch 90 \t Training Loss: 0.022242591017857194 \t Validation Loss: 0.1394549012184143 \n","\n","\n"," Epoch 100 \t Training Loss: 0.02221729272666077 \t Validation Loss: 0.14063692092895508 \n","\n","\n"," Epoch 110 \t Training Loss: 0.022195561478535335 \t Validation Loss: 0.14166976511478424 \n","\n","\n"," Epoch 120 \t Training Loss: 0.022176643833518028 \t Validation Loss: 0.14257405698299408 \n","\n","\n"," Epoch 130 \t Training Loss: 0.022159990078459185 \t Validation Loss: 0.1433681696653366 \n","\n","\n"," Epoch 140 \t Training Loss: 0.022145190664256614 \t Validation Loss: 0.1440678983926773 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 03:00:50,070]\u001b[0m Trial 21 finished with value: 54.31691110850975 and parameters: {'batch_size': 512, 'learning_rate': 0.02810008830238233, 'hidden_size': 16, 'num_layers': 1}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.07866280525922775\n"," \n","SMAPE : 54.31691110850975\n","\n"," Epoch 0 \t Training Loss: 0.05932383673886458 \t Validation Loss: 0.3931657671928406 \n","\n","Validation Loss Decreased(inf--->0.393166) \t Saving The Model\n","Validation Loss Decreased(0.393166--->0.303679) \t Saving The Model\n","Validation Loss Decreased(0.303679--->0.194957) \t Saving The Model\n","Validation Loss Decreased(0.194957--->0.141637) \t Saving The Model\n","Validation Loss Decreased(0.141637--->0.112936) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.02392453607171774 \t Validation Loss: 0.08832439035177231 \n","\n","Validation Loss Decreased(0.112936--->0.088324) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.02292583300732076 \t Validation Loss: 0.12933389842510223 \n","\n","\n"," Epoch 30 \t Training Loss: 0.02272762133119007 \t Validation Loss: 0.12854336202144623 \n","\n","\n"," Epoch 40 \t Training Loss: 0.022601786147182185 \t Validation Loss: 0.13383135199546814 \n","\n","\n"," Epoch 50 \t Training Loss: 0.022511790428931516 \t Validation Loss: 0.13770531117916107 \n","\n","\n"," Epoch 60 \t Training Loss: 0.02244815044105053 \t Validation Loss: 0.14092035591602325 \n","\n","\n"," Epoch 70 \t Training Loss: 0.022397056842843693 \t Validation Loss: 0.14340300858020782 \n","\n","\n"," Epoch 80 \t Training Loss: 0.02235549719383319 \t Validation Loss: 0.14521917700767517 \n","\n","\n"," Epoch 90 \t Training Loss: 0.02232085952224831 \t Validation Loss: 0.1465383917093277 \n","\n","\n"," Epoch 100 \t Training Loss: 0.02229096135124564 \t Validation Loss: 0.14754779636859894 \n","\n","\n"," Epoch 110 \t Training Loss: 0.022264533598596852 \t Validation Loss: 0.14833031594753265 \n","\n","\n"," Epoch 120 \t Training Loss: 0.022240872960537672 \t Validation Loss: 0.1488078534603119 \n","\n","\n"," Epoch 130 \t Training Loss: 0.022219543655713398 \t Validation Loss: 0.1490270048379898 \n","\n","\n"," Epoch 140 \t Training Loss: 0.02219984509671728 \t Validation Loss: 0.14917220175266266 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 03:00:56,173]\u001b[0m Trial 22 finished with value: 35.48730720898065 and parameters: {'batch_size': 512, 'learning_rate': 0.020207372232107036, 'hidden_size': 32, 'num_layers': 2}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.08832439035177231\n"," \n","SMAPE : 35.48730720898065\n","\n"," Epoch 0 \t Training Loss: 0.053524986699630975 \t Validation Loss: 0.027636573743075134 \n","\n","Validation Loss Decreased(inf--->0.027637) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.037876852809858974 \t Validation Loss: 0.14575910866260527 \n","\n","Validation Loss Decreased(0.027637--->0.015691) \t Saving The Model\n","Validation Loss Decreased(0.015691--->0.015197) \t Saving The Model\n","Validation Loss Decreased(0.015197--->0.014946) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.01571297268892522 \t Validation Loss: 0.016622014064341783 \n","\n","Validation Loss Decreased(0.014946--->0.012828) \t Saving The Model\n","Validation Loss Decreased(0.012828--->0.011753) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.017138857960526367 \t Validation Loss: 0.012452332861721515 \n","\n","\n"," Epoch 40 \t Training Loss: 0.016519554892875023 \t Validation Loss: 0.013948499038815499 \n","\n","\n"," Epoch 50 \t Training Loss: 0.006540636885620188 \t Validation Loss: 0.01787545089609921 \n","\n","\n"," Epoch 60 \t Training Loss: 0.016318033587594982 \t Validation Loss: 0.02310100756585598 \n","\n","Validation Loss Decreased(0.011753--->0.011339) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.007882158292341046 \t Validation Loss: 0.017091819923371075 \n","\n","\n"," Epoch 80 \t Training Loss: 0.0032522954385058255 \t Validation Loss: 0.02103859493508935 \n","\n","\n"," Epoch 90 \t Training Loss: 0.004896533707551498 \t Validation Loss: 0.01989800571464002 \n","\n","\n"," Epoch 100 \t Training Loss: 0.009317466682114172 \t Validation Loss: 0.02154006324708462 \n","\n","\n"," Epoch 110 \t Training Loss: 0.022577740607084708 \t Validation Loss: 0.07235899046063424 \n","\n","Validation Loss Decreased(0.011339--->0.010361) \t Saving The Model\n","Validation Loss Decreased(0.010361--->0.009958) \t Saving The Model\n","\n"," Epoch 120 \t Training Loss: 0.00812115343469486 \t Validation Loss: 0.03248782716691494 \n","\n","\n"," Epoch 130 \t Training Loss: 0.019851296128035757 \t Validation Loss: 0.025653068907558918 \n","\n","\n"," Epoch 140 \t Training Loss: 0.011556143188681745 \t Validation Loss: 0.010972552280873061 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 03:01:12,492]\u001b[0m Trial 23 finished with value: 32.6686359291115 and parameters: {'batch_size': 64, 'learning_rate': 0.07338368204890808, 'hidden_size': 32, 'num_layers': 1}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.009957685880362988\n"," \n","SMAPE : 32.6686359291115\n","\n"," Epoch 0 \t Training Loss: 93.23113359902054 \t Validation Loss: 25.367993545532226 \n","\n","Validation Loss Decreased(inf--->25.367994) \t Saving The Model\n","Validation Loss Decreased(25.367994--->0.755619) \t Saving The Model\n","Validation Loss Decreased(0.755619--->0.101930) \t Saving The Model\n","Validation Loss Decreased(0.101930--->0.026514) \t Saving The Model\n","Validation Loss Decreased(0.026514--->0.024420) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.048406721669016405 \t Validation Loss: 0.04176382776349783 \n","\n","Validation Loss Decreased(0.024420--->0.024130) \t Saving The Model\n","\n"," Epoch 20 \t Training Loss: 0.18812687755562366 \t Validation Loss: 0.17407711409032345 \n","\n","\n"," Epoch 30 \t Training Loss: 0.05945997220696882 \t Validation Loss: 0.02412647856399417 \n","\n","Validation Loss Decreased(0.024130--->0.024126) \t Saving The Model\n","\n"," Epoch 40 \t Training Loss: 0.2162572352302959 \t Validation Loss: 0.24877162724733354 \n","\n","\n"," Epoch 50 \t Training Loss: 0.12385121902625543 \t Validation Loss: 0.024253233475610612 \n","\n","\n"," Epoch 60 \t Training Loss: 0.0777505457343068 \t Validation Loss: 0.025226713810116052 \n","\n","\n"," Epoch 70 \t Training Loss: 0.270010597052169 \t Validation Loss: 0.285664501786232 \n","\n","\n"," Epoch 80 \t Training Loss: 0.13523846314637922 \t Validation Loss: 0.03413346111774444 \n","\n","\n"," Epoch 90 \t Training Loss: 0.08435715251398505 \t Validation Loss: 0.024612627644091844 \n","\n","\n"," Epoch 100 \t Training Loss: 0.29161975896859077 \t Validation Loss: 0.21538247764110566 \n","\n","\n"," Epoch 110 \t Training Loss: 0.33115561552840517 \t Validation Loss: 0.034274401143193245 \n","\n","\n"," Epoch 120 \t Training Loss: 0.11336257118964568 \t Validation Loss: 0.08918079622089863 \n","\n","\n"," Epoch 130 \t Training Loss: 0.1294445391300542 \t Validation Loss: 0.03657811973243952 \n","\n","\n"," Epoch 140 \t Training Loss: 0.31602592784947775 \t Validation Loss: 0.2209312029182911 \n","\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-06-19 03:02:48,495]\u001b[0m Trial 24 finished with value: 92.00227840470889 and parameters: {'batch_size': 64, 'learning_rate': 0.07240514123085036, 'hidden_size': 512, 'num_layers': 1}. Best is trial 7 with value: 25.80798653882269.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["best loss for the trial =  0.02412647856399417\n"," \n","SMAPE : 92.00227840470889\n"]}]},{"cell_type":"code","source":["joblib.dump(study, '/content/drive/MyDrive/Colab Notebooks/실무인증/Data/lstm_optuna_key_220619_01.pkl')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GiIa9yM6A1k4","executionInfo":{"status":"ok","timestamp":1655608313520,"user_tz":-540,"elapsed":403,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"eefb0749-5b46-4e2c-ea92-e547b61414f5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Colab Notebooks/실무인증/Data/lstm_optuna_key_220619_01.pkl']"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","source":["study = joblib.load(\"/content/drive/MyDrive/Colab Notebooks/실무인증/Data/lstm_optuna_key_220619_01.pkl\")\n","print(\"Best trial until now:\")\n","print(\" Value: \", study.best_trial.value)\n","print(\" Params: \")\n","for key, value in study.best_trial.params.items():\n","    print(f\"    {key}: {value}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"268hIGwJVkq9","executionInfo":{"status":"ok","timestamp":1655363662253,"user_tz":-540,"elapsed":2,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"744e9b4e-8358-444c-92f9-a52128804eab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best trial until now:\n"," Value:  5.477795508716941\n"," Params: \n","    max_epochs: 50\n","    learning_rate: 0.005659123649008913\n","    hidden_size: 256\n"]}]},{"cell_type":"code","source":["optuna.visualization.matplotlib.plot_param_importances(study)\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"b-6nLQBPVoZ9","executionInfo":{"status":"ok","timestamp":1655608343339,"user_tz":-540,"elapsed":1034,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"df00cd8f-9243-4a2e-ff0a-366404be7a37"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["print(\"Number of finished trials: \", len(study.trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))\n","\n","\n","# optuna.visualization.matplotlib.plot_param_importances(study)\n","# plt.show()\n","\n","\n","optuna.visualization.matplotlib.plot_optimization_history(study)\n","plt.show()"],"metadata":{"id":"2SWG-vK0huDm","colab":{"base_uri":"https://localhost:8080/","height":446},"executionInfo":{"status":"ok","timestamp":1655608347511,"user_tz":-540,"elapsed":492,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"a1ff7b4b-a202-4569-c097-abe71374457f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of finished trials:  25\n","Best trial:\n","  Value:  25.80798653882269\n","  Params: \n","    batch_size: 64\n","    learning_rate: 0.014949675790007754\n","    hidden_size: 16\n","    num_layers: 4\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## Plot Results"],"metadata":{"id":"4UVQqgCk_NVo"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","\n","log_interval = 10\n","num_classes = 1 # parameter에서 빼서 상수로 설정\n","num_epochs = 100 # parameter에서 빼서 상수로 설정\n","\n","train_dl, val_dl, input_size = get_data_loader(x, y,  study.best_params['batch_size'])\n","\n","model = LSTM(num_classes=num_classes,\n","                input_size=input_size, \n","                hidden_size=study.best_params['hidden_size'], \n","                num_layers=study.best_params['num_layers'])\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","optimizer = optim.Adam(model.parameters(), lr=study.best_params['learning_rate'])\n","criterion = torch.nn.MSELoss()\n","best_loss = train(log_interval, model, train_dl, val_dl, optimizer, criterion,  num_epochs)\n","\n","print('best loss for the trial = ', best_loss)\n","predict_data = []\n","\n","# 여기서 x는 (sample, lookback_length, 1)의 크기를 지님. 따라서, 제일 앞의 시점을 제거하려면, x[:, -1, :]이 되어야 함\n","x_pred = np.expand_dims(x_for_metric, 0)  # Inference에 사용할 lookback data를 x_pred로 지정. 앞으로 x_pred를 하나씩 옮겨 가면서 inference를 할 예정\n","\n","# print('-----------------------y shape before loop = ', y.shape)\n","for j, i in enumerate(range(max_prediction_length)):\n","\n","    # feed the last forecast back to the model as an input\n","    x_pred = np.append( x_pred[:, 1:, :], np.expand_dims(y_for_metric[j, :], (0,2)), axis=1)\n","\n","    # print(f'After update data = {x_pred.shape}')\n","    xt_pred = torch.Tensor(x_pred)\n","\n","    if torch.cuda.is_available():\n","        xt_pred = xt_pred.cuda()\n","\n","    # generate the next forecast\n","    yt_pred = model(xt_pred)\n","\n","    # print(f'model result yt_pred = {yt_pred.shape}')\n","    # tensor to array\n","    # x_pred = xt_pred.cpu().detach().numpy()\n","    y_pred = yt_pred.cpu().detach().numpy()\n","\n","    # save the forecast\n","    predict_data.append(y_pred)\n","\n","\n","# transform the forecasts back to the original scale\n","predict_data = np.array(predict_data).reshape(-1, 1)\n","SMAPE = smape(y_for_metric, predict_data)\n","print(f' \\nSMAPE : {SMAPE}')\n"],"metadata":{"id":"rNdPlJCASiX0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655608380118,"user_tz":-540,"elapsed":29204,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"6f171894-8951-4b4b-87a4-7161465eaa5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0 \t Training Loss: 0.022796441253740342 \t Validation Loss: 0.18827576115727424 \n","\n","Validation Loss Decreased(inf--->0.188276) \t Saving The Model\n","Validation Loss Decreased(0.188276--->0.158746) \t Saving The Model\n","\n"," Epoch 10 \t Training Loss: 0.028290396140073427 \t Validation Loss: 0.37542605102062226 \n","\n","\n"," Epoch 20 \t Training Loss: 0.0112752937246114 \t Validation Loss: 0.1007685698568821 \n","\n","Validation Loss Decreased(0.158746--->0.100769) \t Saving The Model\n","\n"," Epoch 30 \t Training Loss: 0.022714841258130038 \t Validation Loss: 0.3167910575866699 \n","\n","\n"," Epoch 40 \t Training Loss: 0.012788764212746172 \t Validation Loss: 0.1736393839120865 \n","\n","Validation Loss Decreased(0.100769--->0.099226) \t Saving The Model\n","Validation Loss Decreased(0.099226--->0.096381) \t Saving The Model\n","\n"," Epoch 50 \t Training Loss: 0.012196027746540494 \t Validation Loss: 0.15084692537784578 \n","\n","Validation Loss Decreased(0.096381--->0.052638) \t Saving The Model\n","\n"," Epoch 60 \t Training Loss: 0.0068195171421393756 \t Validation Loss: 0.07072418108582497 \n","\n","Validation Loss Decreased(0.052638--->0.050540) \t Saving The Model\n","Validation Loss Decreased(0.050540--->0.025829) \t Saving The Model\n","\n"," Epoch 70 \t Training Loss: 0.0037355768341512884 \t Validation Loss: 0.025130067579448225 \n","\n","Validation Loss Decreased(0.025829--->0.025130) \t Saving The Model\n","Validation Loss Decreased(0.025130--->0.024111) \t Saving The Model\n","Validation Loss Decreased(0.024111--->0.022873) \t Saving The Model\n","\n"," Epoch 80 \t Training Loss: 0.0045904308521130584 \t Validation Loss: 0.023237980995327236 \n","\n","Validation Loss Decreased(0.022873--->0.022678) \t Saving The Model\n","Validation Loss Decreased(0.022678--->0.022629) \t Saving The Model\n","Validation Loss Decreased(0.022629--->0.022236) \t Saving The Model\n","Validation Loss Decreased(0.022236--->0.022115) \t Saving The Model\n","\n"," Epoch 90 \t Training Loss: 0.004899038246367127 \t Validation Loss: 0.021809390000998973 \n","\n","Validation Loss Decreased(0.022115--->0.021809) \t Saving The Model\n","Validation Loss Decreased(0.021809--->0.021734) \t Saving The Model\n","Validation Loss Decreased(0.021734--->0.021557) \t Saving The Model\n","Validation Loss Decreased(0.021557--->0.021537) \t Saving The Model\n","Validation Loss Decreased(0.021537--->0.021439) \t Saving The Model\n","best loss for the trial =  0.02143903449177742\n"," \n","SMAPE : 23.662659829033803\n"]}]},{"cell_type":"code","source":["predict = scaler.inverse_transform(predict_data)\n","pred_df = pd.concat([data_p[scale_cols], pd.DataFrame(predict, columns=['trade_price'])], axis=0).reset_index(drop=True)\n","pred_df = pred_df.rename(columns = {'trade_price':'pred_price'})\n","data_df = pd.concat([data, pred_df], axis=1)\n","\n","plt.plot(data_df['pred_price'][len(data_df)-lookback_length*2:], label='prediction')\n","plt.plot(data_df['trade_price'][len(data_df)-lookback_length*2:], label='actual')\n","plt.suptitle('Timeseries Prediction')\n","plt.axvline(x = len(data_df) - len(predict_data), c = 'g', linestyle = '--')\n","plt.legend()\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":294},"id":"BPIzb9vppSSG","executionInfo":{"status":"ok","timestamp":1655609057110,"user_tz":-540,"elapsed":12,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"7388b3be-78b4-4d73-c0fd-1ca34520bc56"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["data_df.loc[training_data_max:,:]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"id":"8ugA0Dk1-CnM","executionInfo":{"status":"ok","timestamp":1655609061176,"user_tz":-540,"elapsed":422,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"67af21a7-e82c-4e5e-ab1c-cfdaad3b4855"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           date  trade_price  pred_price\n","1687 2022-05-09    3038000.0  2740677.75\n","1688 2022-05-10    3095000.0  2789122.50\n","1689 2022-05-11    2884000.0  2736573.25\n","1690 2022-05-12    2648000.0  2653853.75\n","1691 2022-05-13    2694000.0  2677715.00\n","1692 2022-05-14    2733000.0  2694038.00\n","1693 2022-05-15    2806000.0  2723763.75\n","1694 2022-05-16    2660000.0  2659486.75\n","1695 2022-05-17    2695000.0  2678051.00\n","1696 2022-05-18    2472000.0  2537104.00\n","1697 2022-05-19    2583000.0  2589392.75\n","1698 2022-05-20    2538000.0  2562935.75\n","1699 2022-05-21    2552000.0  2563094.75\n","1700 2022-05-22    2627000.0  2608472.75\n","1701 2022-05-23    2533000.0  2554477.00\n","1702 2022-05-24    2525000.0  2539622.50\n","1703 2022-05-25    2494000.0  2511062.50\n","1704 2022-05-26    2314000.0  2366956.75\n","1705 2022-05-27    2225000.0  2287215.75\n","1706 2022-05-28    2286000.0  2303007.25"],"text/html":["\n","  <div id=\"df-f75fa855-52b2-4253-a0af-0197fa02aa72\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>trade_price</th>\n","      <th>pred_price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1687</th>\n","      <td>2022-05-09</td>\n","      <td>3038000.0</td>\n","      <td>2740677.75</td>\n","    </tr>\n","    <tr>\n","      <th>1688</th>\n","      <td>2022-05-10</td>\n","      <td>3095000.0</td>\n","      <td>2789122.50</td>\n","    </tr>\n","    <tr>\n","      <th>1689</th>\n","      <td>2022-05-11</td>\n","      <td>2884000.0</td>\n","      <td>2736573.25</td>\n","    </tr>\n","    <tr>\n","      <th>1690</th>\n","      <td>2022-05-12</td>\n","      <td>2648000.0</td>\n","      <td>2653853.75</td>\n","    </tr>\n","    <tr>\n","      <th>1691</th>\n","      <td>2022-05-13</td>\n","      <td>2694000.0</td>\n","      <td>2677715.00</td>\n","    </tr>\n","    <tr>\n","      <th>1692</th>\n","      <td>2022-05-14</td>\n","      <td>2733000.0</td>\n","      <td>2694038.00</td>\n","    </tr>\n","    <tr>\n","      <th>1693</th>\n","      <td>2022-05-15</td>\n","      <td>2806000.0</td>\n","      <td>2723763.75</td>\n","    </tr>\n","    <tr>\n","      <th>1694</th>\n","      <td>2022-05-16</td>\n","      <td>2660000.0</td>\n","      <td>2659486.75</td>\n","    </tr>\n","    <tr>\n","      <th>1695</th>\n","      <td>2022-05-17</td>\n","      <td>2695000.0</td>\n","      <td>2678051.00</td>\n","    </tr>\n","    <tr>\n","      <th>1696</th>\n","      <td>2022-05-18</td>\n","      <td>2472000.0</td>\n","      <td>2537104.00</td>\n","    </tr>\n","    <tr>\n","      <th>1697</th>\n","      <td>2022-05-19</td>\n","      <td>2583000.0</td>\n","      <td>2589392.75</td>\n","    </tr>\n","    <tr>\n","      <th>1698</th>\n","      <td>2022-05-20</td>\n","      <td>2538000.0</td>\n","      <td>2562935.75</td>\n","    </tr>\n","    <tr>\n","      <th>1699</th>\n","      <td>2022-05-21</td>\n","      <td>2552000.0</td>\n","      <td>2563094.75</td>\n","    </tr>\n","    <tr>\n","      <th>1700</th>\n","      <td>2022-05-22</td>\n","      <td>2627000.0</td>\n","      <td>2608472.75</td>\n","    </tr>\n","    <tr>\n","      <th>1701</th>\n","      <td>2022-05-23</td>\n","      <td>2533000.0</td>\n","      <td>2554477.00</td>\n","    </tr>\n","    <tr>\n","      <th>1702</th>\n","      <td>2022-05-24</td>\n","      <td>2525000.0</td>\n","      <td>2539622.50</td>\n","    </tr>\n","    <tr>\n","      <th>1703</th>\n","      <td>2022-05-25</td>\n","      <td>2494000.0</td>\n","      <td>2511062.50</td>\n","    </tr>\n","    <tr>\n","      <th>1704</th>\n","      <td>2022-05-26</td>\n","      <td>2314000.0</td>\n","      <td>2366956.75</td>\n","    </tr>\n","    <tr>\n","      <th>1705</th>\n","      <td>2022-05-27</td>\n","      <td>2225000.0</td>\n","      <td>2287215.75</td>\n","    </tr>\n","    <tr>\n","      <th>1706</th>\n","      <td>2022-05-28</td>\n","      <td>2286000.0</td>\n","      <td>2303007.25</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f75fa855-52b2-4253-a0af-0197fa02aa72')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f75fa855-52b2-4253-a0af-0197fa02aa72 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f75fa855-52b2-4253-a0af-0197fa02aa72');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["plt.plot(data_df.loc[training_data_max:, 'pred_price'], label='predict')\n","plt.plot(data_df.loc[training_data_max:, 'trade_price'], label='actual')\n","plt.suptitle('Timeseries Prediction')\n","plt.legend()\n","plt.show();"],"metadata":{"id":"tfOoGb4JQOoi","colab":{"base_uri":"https://localhost:8080/","height":294},"executionInfo":{"status":"ok","timestamp":1655608870440,"user_tz":-540,"elapsed":519,"user":{"displayName":"이기","userId":"02906280205536551697"}},"outputId":"71d3b675-74e7-4498-99d9-8ba2f519f27e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["%reload_ext tensorboard\n","%tensorboard --logdir=runs/"],"metadata":{"id":"oeIbqVgJerQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2_vaYEzw-ve9"},"execution_count":null,"outputs":[]}]}